<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://kristosh.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kristosh.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-05-12T14:53:18+00:00</updated><id>https://kristosh.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">a post with table of contents</title><link href="https://kristosh.github.io/blog/2023/table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents" /><published>2023-03-20T15:59:00+00:00</published><updated>2023-03-20T15:59:00+00:00</updated><id>https://kristosh.github.io/blog/2023/table-of-contents</id><content type="html" xml:base="https://kristosh.github.io/blog/2023/table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents in the beginning of the post.</p>

<h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2>

<p>To add a table of contents to a post, simply add</p>
<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">beginning</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div>
<p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post.</p>

<h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h2 id="table-of-contents-options">Table of Contents Options</h2>

<p>If you want to learn more about how to customize the table of contents, you can check the <a href="https://github.com/toshimaru/jekyll-toc">jekyll-toc</a> repository.</p>

<h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><summary type="html"><![CDATA[an example of a blog post with table of contents]]></summary></entry><entry><title type="html">Posthoc explainability in AI</title><link href="https://kristosh.github.io/blog/2023/Posthoc-XAI/" rel="alternate" type="text/html" title="Posthoc explainability in AI" /><published>2023-02-11T00:00:00+00:00</published><updated>2023-02-11T00:00:00+00:00</updated><id>https://kristosh.github.io/blog/2023/Posthoc-XAI</id><content type="html" xml:base="https://kristosh.github.io/blog/2023/Posthoc-XAI/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>This workshop’s core focus is to analyze a branch of explainable &amp; interpretable AI (XAI) called posthoc XAI. We will analyze theory, taxonomy, applications, shortcomings of posthoc XAI approaches and apply them on image classification using popular CNN architectures and explain their <em>black box</em> nature. Part of the assessemnet for this tutorial/workshop, will be some research questions that needs be answered by you. These questions can be found all over this notebook and will be summarized them at the end.</p>

<p>The learning objectives (ILOs) for this tutorial can be listed as follows:</p>
<ul>
  <li><em>Learning basic terminology for XAI and introduced to one possible taxonomy</em>,</li>
  <li><em>Getting familiar with several Posthoc techniques for XAI (by a thorough list of references)</em>,</li>
  <li><em>Test &amp; compare these approaches using CNN classifier</em>,</li>
  <li><em>Investigate and discover potential shortcoming of these techniques</em>,</li>
  <li><em>Discovering means for mitigating problems in XAI methods</em>.</li>
</ul>

<font color="red"><b>TOSUBMIT</b></font>
<p>Please submit answers on the questions marked “ToSubmit” in one workshop-report, together with your answers to the ToSubmit questions in the Lab 1 notebook. Use a single pdf with max. of 2 pages. Please submit as required via canvas/assignments. Include your name + student number. This report will be graded.</p>

<font color="red"><b>TODO</b></font>
<p>The code below is ready to run for the largest part. In order to ensure that you do an effort to understand the code we do ask you to finalize a few parts of the code, usually just one or a few lines of code are necessary. You do not have to submit the code.</p>

<h2 id="introduction-to-xai">Introduction to XAI</h2>

<p>A ill-famed shortcoming in Deep Learning models is their <em>black-box</em> or <em>opaque</em> nature. Reasoning on their behavior, decisions and predictions is an fundamental task when developers debugging their models but also when AI users assessing the level of <em>trustworthiness</em> that they have on these models. It is really fundamental, especially when one is planning to take actions using these systems, or when deploying AI in high-stake real-world applications (such as law, finance, tumor analysis etc.). Understanding of the inner <em>black-box</em> mechanisms also provides insights into the model, which can be used to transform an <em>untrustworthy</em> model or prediction into a <em>trustworthy</em> one. Finally, GDPR regulations require DL models to be transparent for reasons of fairness.</p>

<h3 id="trust-ai">Trust AI</h3>

<p>There is a widespread research in what it means a user to trust AI models and how explainability &amp; interpretability can help mitigating opaqueness in AI. For simplicity reasons, we will employ here a really an basic definition: <em>trusting an AI algorithm means to be in a shape to anticipate its outcome and verify its performance (contractual trust)</em>.The following paper provide some good analysis on this aspect [Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI]. If users do not trust a model or a prediction, they will probably not use it.</p>

<p>We can seperate the notation of trust into two parts:</p>
<ul>
  <li>Trusting a prediction, i.e. whether a user trusts an individual prediction (the output of the model given a particular input) sufficiently to take some action based on it.</li>
  <li>Trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed (for the whole input space).</li>
</ul>

<h3 id="interpretability-accuracy-trade-off">Interpretability accuracy trade-off</h3>

<p>With the increasing popularity of AI the introduced approaches methodologies became more efficient but at the same time, increased their complexity. Think as an example the simple rule-based approaches in taking decisions. While they offer a really easy way to be explained, however, oftently, they lack of good accuracy in real-world problems. As it is apparent by the figure, there is a tradeoff between the achieved accuracy and the interpretability of the well-known machine learning approaches.</p>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-02-11-Posthoc-XAI/tradeoff.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="explainability--interpretability">Explainability &amp; interpretability</h3>

<p>By explaining a <em>prediction</em>, we define the visual artifacts that provide qualitative understanding of the relationship between the instance’s components (e.g. words in text, patches in an image) and the model’s prediction. We argue that explaining predictions is an important aspect in getting humans to trust and use machine learning effectively, if the explanations are faithful and intelligible. <em>To increase the trust in predictions and models Explainable and Interpretable AI (XAI) came at the rescue</em>.</p>

<p>At this point, I would like to adotp a way to distinguish the following two terms: <em>interpretability</em> and <em>explaibility</em> using Ajay Thampi’s Interpretable AI book (https://www.manning.com/books/interpretable-ai):</p>

<ul>
  <li><em>Interpretability</em>: It is the degree to which we can consistently estimate what a model will predict given an input, understand how the model came up with the prediction, understand how the prediction changes with changes in the input or algorithmic parameters and finally understand when the model has made a mistake. Interpretability is mostly discernible by experts who are either building, deploying or using the AI system and these techniques are building blocks that will help you get to explainability.</li>
  <li><em>Explainability:</em> Goes beyond interpretability in that it helps us understand in a human-readable form how and why a model came up with a prediction. It explains the internal mechanics of the system in human terms with the intent to reach a much wider audience. Explainability requires interpretability as building blocks and also looks to other fields and areas such as Human-Computer Interaction (HCI), law and ethics.</li>
</ul>

<h3 id="mitigating-algorithmic-bias-using-xai">Mitigating algorithmic bias using XAI</h3>

<p>Interpretability is a useful debugging tool for detecting bias in machine learning models. It might happen that the machine learning model you have trained for automatic approval or rejection of credit applications discriminates against a minority that has been historically disenfranchised. Your main goal is to grant loans only to people who will eventually repay them. The incompleteness of the problem formulation in this case lies in the fact that you not only want to minimize loan defaults, but are also obliged not to discriminate on the basis of certain demographics. This is an additional constraint that is part of your problem formulation (granting loans in a low-risk and compliant way) that is not covered by the loss function the machine learning model was optimized for.</p>

<p>Explainability techniques could help identify whether the factors considered in a decision reflect bias and could enable more accountability than in human decision making, which typically cannot be subjected to such rigorous probing.</p>

<ul>
  <li>Bias in online recruitment tools</li>
  <li>Bias in word associations</li>
  <li>Bias in online ads</li>
  <li>Bias in facial recognition technology</li>
  <li>Bias in criminal justice algorithms</li>
</ul>

<h3 id="taxonomy-of-xai-techniques">Taxonomy of XAI techniques</h3>

<p>In the following picture you can find a possible way to categorize XAI methods:</p>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2023-02-11-Posthoc-XAI/XAI_categories.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Firstly, XAI methods can be categorized into model-based and posthoc approaches. The former is trying to render the model itself interpretable and thus explain its behavior in clear and easy manner. While the later category, after having generate a classificaiton model that behaves as a black box and its too complicate to be inherently explain, aims at explaning the bevavior of the model after the training procedure.</p>

<p>The posthoc methods could be also divided between global and local methods. The global methods trying to explain the model around a specifc input sample while the global techniques are trying to do that on a global scale.</p>

<p>Another way to distinguinsh XAI models is between Model-specific or model-agnostic? Model-specific interpretation tools are limited to specific model classes. The interpretation of regression weights in a linear model is a model-specific interpretation, since – by definition – the interpretation of intrinsically interpretable models is always model-specific. Tools that only work for the interpretation of e.g. neural networks are model-specific. Model-agnostic tools can be used on any machine learning model and are applied after the model has been trained (post hoc). These agnostic methods usually work by analyzing feature input and output pairs. By definition, these methods cannot have access to model internals such as weights or structural information.</p>

<p>Local or global? Does the interpretation method explain an individual prediction or the entire model behavior? Or is the scope somewhere in between? Read more about the scope criterion in the next section.</p>

<p>In this tutorial we will focus in several posthoc explanation and its subcategories for vision. We can categorize posthoc methods in three basic categories:</p>
<ul>
  <li><em>Gradient based methods</em> Many methods compute the gradient of the prediction (or classification score) with respect to the input features. The gradient-based methods (of which there are many) mostly differ in how the gradient is computed. Since in these methods we need to calcualte the gradient of the models, thus, we need to have access to the models themselves, therefore, model-specific techniques.</li>
  <li>Surrogate methods.</li>
  <li>Perturbation-based methods.</li>
  <li><em>Feature of pixel attribution methods</em> highlight the features or pixels that were relevant for a certain image classification by a neural network.</li>
</ul>

<p>In this tutorial, tou will apply a few techniques for feature attribution: 
<strong>Feature attribution</strong></p>
<ul>
  <li><strong>Leave one out (LOO)</strong></li>
  <li><strong>LIME</strong> method</li>
  <li><strong>XAI counterfactuals</strong></li>
  <li><strong>SHAP</strong>
<strong>Gradient-based:</strong></li>
  <li><strong>Saliency mapping</strong>: use gradients to understand what image pixels are most important for classification</li>
  <li><strong>Integrated gradients</strong> for the same purpose</li>
  <li><strong>GradCam</strong> to understand what areas of the image are important for classification.</li>
</ul>

<h2 id="gradient-based-methods">Gradient-based methods</h2>

<p>Before starting with the explanation of the gradient-based methodologies, we provide some useful code for all the necessarry packages loacing a pre-trained VGG model (in Imagenet) but also code to load a image for a local directory in PyTorch. You can access the code of this tutorial in the following <a href="https://colab.research.google.com/drive/1zWmtpOTXfxv1Hxwl7G5heU73vz90iNhl?usp=sharing">google colab link</a>.</p>

<p>The packages that you will need to import are the following ones:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># set-up environment
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">os</span>  <span class="c1"># necessary
</span><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="c1">#from google.colab.patches import cv2_imshow   # specific for colab
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'KMP_DUPLICATE_LIB_OK'</span><span class="p">]</span><span class="o">=</span><span class="s">'True'</span>
</code></pre></div></div>

<h3 id="load-a-pretrained-model">Load a pretrained model</h3>

<p>We will make use of the VGG19 CNN network and ImageNet.</p>

<ul>
  <li>ImageNet is a large collection of images.</li>
  <li>VGG19 is a convolutional neural network architecture.</li>
  <li>
    <p>We can load a version that is trained on ImageNet and that can detect objects in 1000 classes.</p>
  </li>
  <li>Read about and understand VGG ConvNet and Imagenet for background.</li>
</ul>

<p>The first step is that using the pytorch library, we load the pretrained version of VGG19.</p>

<p>Since we will not train the model we set the model in evaluation mode.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load model
# model_type = 'vgg19'
</span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">vgg19</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># run it on a GPU if available:
</span><span class="n">cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'cuda:'</span><span class="p">,</span> <span class="n">cuda</span><span class="p">,</span> <span class="s">'device:'</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># set model to evaluation
</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
</code></pre></div></div>

<p>The output should look like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Output</span> <span class="n">exceeds</span> <span class="n">the</span> <span class="n">size</span> <span class="n">limit</span><span class="p">.</span> <span class="n">Open</span> <span class="n">the</span> <span class="n">full</span> <span class="n">output</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">text</span> <span class="nf">editorVGG</span><span class="p">(</span>
  <span class="p">(</span><span class="n">features</span><span class="p">):</span> <span class="nc">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="nc">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">7</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">8</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">9</span><span class="p">):</span> <span class="nc">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">10</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">11</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">12</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">13</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">14</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">15</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">16</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">17</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">18</span><span class="p">):</span> <span class="nc">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">19</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">20</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">21</span><span class="p">):</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">22</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="p">...</span>
    <span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<h3 id="load-and-preprocess-the-images">Load and preprocess the images:</h3>

<p>We have provided a few images of wildlife, but please also use you own imagery. Set the path to your data-file and load an image.</p>

<p>VGG-19 works best if image is normalised. Image should also be in the correct tensor format.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">cuda</span><span class="p">):</span>
    <span class="c1"># Students should transpose the image to the correct tensor format. 
</span>    <span class="c1"># Students should ensure that gradient for input is calculated       
</span>    <span class="c1"># set the GPU device
</span>    <span class="k">if</span> <span class="n">cuda</span><span class="p">:</span>
        <span class="n">torch_device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">'cuda:0'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">torch_device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>

    <span class="c1"># normalise for ImageNet
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span> <span class="o">/</span> <span class="mi">255</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="p">(</span><span class="n">obs</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>

    <span class="c1"># make tensor format that keeps track of gradient
</span>    <span class="c1"># BEGIN for students to do
</span>    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>       
    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="n">obs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch_device</span><span class="p">)</span>
    <span class="c1"># END for students to do
</span>    <span class="k">return</span> <span class="n">obs_tensor</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="c1">#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook
</span><span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># read the image and convert it - Set your pathto the image
</span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="s">'elephant-zebra.png'</span><span class="p">)</span>
<span class="c1">#img = cv2.imread(datafiles+ 'R.png')
#img = cv2.imread(datafiles+ 'elephant/Elephant2.jpeg')
# img = cv2.imread(datafiles+ 'shark/Shark1.jpeg')
</span><span class="nf">if </span><span class="p">(</span><span class="nf">type</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
  <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
  <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
  <span class="nf">print</span><span class="p">(</span><span class="s">'img:'</span><span class="p">,</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>


<span class="k">else</span><span class="p">:</span>
  <span class="nf">print</span><span class="p">(</span><span class="s">'image not found - set your path to the image'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'white'</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nc">Axes</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_axis_off</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">add_axes</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="predict-class">Predict class</h3>

<p>We can easily predict the class, and the softmax score of that prediction:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">cuda</span><span class="p">):</span>
    <span class="c1"># Makes prediction after preprocessing image 
</span>    <span class="c1"># Note that output should be torch.tensor on cuda
</span>    <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>                        
    <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># calc output from model 
</span>    <span class="k">if</span> <span class="n">target_label_idx</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">target_label_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">output</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">target_label_idx</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span> 
    <span class="k">if</span> <span class="n">cuda</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>                     <span class="c1"># calc prediction
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>           <span class="c1"># gather functionality of pytorch
</span>    <span class="k">return</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">output</span> 

<span class="c1"># test preprocessing
# you can check that the VGG network gives a correct prediction. E.g. 385 and 386 are 'Indian Elephant'and 'African Elephant'
</span><span class="nb">input</span> <span class="o">=</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>          <span class="c1"># preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
</span><span class="n">output</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'output:'</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>TODO 1</strong></p>
<ul>
  <li>Upload at least two images to your directory with the first one containing a single animal and the second image with two animals.</li>
  <li>Run the classifier with both images.</li>
  <li>Look up the predicted categories and the ImageNet labels.</li>
  <li>Look up the indices corresponsing to each of the animals in your images.</li>
</ul>

<p>The following snippets might be useful:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="p">.</span><span class="n">githubusercontent</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">hub</span><span class="o">/</span><span class="n">master</span><span class="o">/</span><span class="n">imagenet_classes</span><span class="p">.</span><span class="n">txt</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="s">"imagenet_classes.txt"</span><span class="p">,</span> <span class="s">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">f</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()]</span>
<span class="nf">print</span><span class="p">(</span><span class="n">categories</span><span class="p">[</span><span class="mi">385</span><span class="p">])</span>
<span class="n">categories</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="s">'Indian elephant'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="compute-the-gradient-with-respect-to-the-input-pixels">Compute the gradient with respect to the input pixels</h3>

<p>Now that we can predict the class of an object, we will try to understand what image pixels are most important for the prediction using <em>feature attribution methods</em>. The first technique that we will make use is the saliency maps. In short this approach determines the gradient of the output w.r.t to the input.</p>

<p>The idea of Saliency maps (called <em> Vanilla Gradient </em> as well), introduced by Simonyan et al. (https://arxiv.org/pdf/1312.6034.pdf) as one of the first pixel attribution approaches. The core idea is really simple and what needs to be done is to calculate the gradient of the loss function for the class we are interested in with respect to the input features. This gives us a map of the size of the input features with negative to positive values.</p>

<p>The recipe for this approach is as follows:</p>

<ul>
  <li><strong>Perform a forward pass</strong> of the image ($\mathbf{x}_0$) of interest using the network $\mathcal{F}(\mathbf{x}_0)$.</li>
  <li><strong>Compute the gradient</strong> of class score of interest with respect to the input image ($\mathbf{x}_0$): $g(\mathbf{x}_0) = \frac{\partial \mathcal{F}}{\partial \mathbf{x}_0} $.</li>
  <li><strong>Visualize the gradients</strong>: You can either show the absolute values or highlight negative and positive contributions separately.</li>
</ul>

<h3 id="the-instructions-for-the-pytorch-code">The instructions for the PyTorch code:</h3>

<p>We have set the model in eval mode, but we can still catch the gradients of the input-image if ask PyTorch to do this and then do some backward calculation. That is what you need to do. So complete the procedure in order that:</p>
<ul>
  <li>Input should be preprocessed (and converted into a torch tensor).</li>
  <li>Set the <mark>required_gradient=True</mark> on the input tensor.</li>
  <li>Calculate the output (with previous method predict).</li>
  <li>Set the gradient to zero and do a backward on the output.</li>
  <li>Gradients w.r.t input can now be found under input.grad</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_outputs_and_gradients</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c1"># Calculates the gradient of the output w.r.t. the input image
</span>    <span class="c1"># The result should be a gradients numpy matrix of same dimensions as the inputs
</span>    <span class="n">predict_idx</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>                             <span class="c1"># for every image
</span>        <span class="nb">input</span> <span class="o">=</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>  
        <span class="nb">input</span><span class="p">.</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span>
        <span class="nf">print </span><span class="p">(</span><span class="n">target_label_idx</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
        <span class="c1"># clear grad
</span>        <span class="n">model</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="c1">## BEGIN student code
</span>        <span class="c1"># Perform a backward pass on the output and collect the gradient w.r.t. the input
</span>        <span class="c1"># Store this gradient in the variable 'gradient' 
</span>        <span class="n">output</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># do backward and gather gradients of input
</span>        <span class="c1">## END student code
</span>        <span class="n">gradients</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">target_label_idx</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># calculate the gradient and the label index
#gradients, label_index = calculate_outputs_and_gradients([img], model, None, cuda)    
</span><span class="n">gradients</span><span class="p">,</span> <span class="n">label_index</span> <span class="o">=</span> <span class="nf">calculate_outputs_and_gradients</span><span class="p">([</span><span class="n">img</span><span class="p">],</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>    
<span class="n">gradients</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">gradients</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="c1">#Note that if target_label_idx == None, the calculate_output)and_gradients function assume:
#            target_label_idx = torch.argmax(output, 1).item()
</span>
<span class="c1"># Please note that the dimensions of gradients are same as dimensions of input
</span><span class="nf">print</span><span class="p">(</span><span class="s">'gradients'</span><span class="p">,</span> <span class="n">gradients</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> 
<span class="c1"># Please note that gradients are positive and negative values
</span><span class="nf">print</span><span class="p">(</span><span class="n">gradients</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">label_index</span><span class="p">)</span>
</code></pre></div></div>

<font color="green"><b>ToDo 2</b></font>
<p>For your image with two animals, consider both ID’s as target_label_idx.</p>
<ul>
  <li>Run the classifier with each ID</li>
  <li>After running the forward pass, compute the gradients</li>
</ul>

<font color="blue"><b>ToThink 2</b></font>
<p>Are the gradients the same when you use different target classes? Why?</p>

<h3 id="visualize-the-gradients">Visualize the gradients</h3>

<p>Try to visualise the image and the saliency map.</p>

<p><strong>Tip:</strong> take absolute values of the gradients and maximize over all three channels.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Retrieve the saliency map and also pick the maximum value from channels on each pixel.
# In this case, we look at dim=2. Recall the shape of gradients (width, height, channel)
</span>
<span class="k">def</span> <span class="nf">plot_gradients</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
  <span class="c1"># plots image (dimensions: Width X Heigth X 3) and gradients (dimensions: Width X Heigh x 3) - both numpy arrays
</span>  <span class="n">saliency</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">gradients</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>       <span class="c1"># takes maximum over 3 color channels                                                 
</span>  <span class="c1"># Visualize the image and the saliency map
</span>  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">saliency</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'hot'</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
  <span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">plot_gradients</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="s">'The Image and Its Saliency Map'</span><span class="p">)</span>

</code></pre></div></div>

<font color="red"><b>ToSubmit 1</b></font>
<p>In your report, include your two images, the two saliency map, the two target  label, the predicted label and the likelihood your models assigns to each label. Add a caption explaining very briefly (1 or 2 sentences) whether there’s a difference and why.</p>

<h3 id="issues-with-saliency-maps-and-vanilla-gradients-saturation">Issues with saliency maps and vanilla gradients (saturation)</h3>

<p>Vanilla Gradient methods, notoriously, are facing saturation problems, as explained in Avanti et al. (2017). When the ReLU is used, and when the activation goes below zero, then, the activation is limited at zero and does not change any more. Hence, the activation is saturated.</p>

<p>Therefore, multiple strategies have been proposed to deal with that issue. One of them is Gradient-weighted Class Activation Map (<em>Grad-Cam</em>) that instead of calculating the gradient to the input image it make use of the last convolutional layer.</p>

<h3 id="gradient-weighted-class-activation-mapping-grad-cam">Gradient-weighted Class Activation Mapping (Grad-CAM)</h3>

<h1 id="4-gradient-weighted-class-activation-mapping-grad-cam"><strong>4 Gradient-weighted Class Activation Mapping (Grad-Cam).</strong></h1>

<p>Unlike saliency maps, in the <strong>Grad-Cam</strong> approach the gradient is not backpropagated all the way back to the image, but (usually) to the last convolutional layer in order to generate a visualization map that highlights important regions of the input.</p>

<p>A naive visualization approach could be the following:</p>

<ul>
  <li>Simply employ the values for each feature map, (of the last convolutional layer),</li>
  <li>Then, average these feature maps and overlay this over our image (by rescaling back to initial size).</li>
</ul>

<p>However, while simple, it is not really helpful approach, since these maps encode information for all classes, while we are interested in a specific class. <strong>Grad-CAM</strong> needs to figure out the importance for each of the $k$ feature map $A_k \in \mathbb{R}^{w \times h}$ ($w$ the width and $h$ the height of the features maps) in respect to our class $c$ of interest.</p>

<p>We have to weight each pixel of each feature map with the gradient before averaging over the feature maps $A_k$. This heatmap is send through the ReLU function which set all negative values to zero. The reason for that is that we are only interested in the parts that contribute to the selected class $c$ and not to other classes. The final feature map is rescaled back to the original image size. We then overlay it over the original image for producing the final visualization.</p>

<p><strong>Grad Cam recipe:</strong></p>

<ul>
  <li>Forward-propagate the input image $\mathbf{x}_0$ through the convolutional VGG19 network by calculating the $\mathcal{F}(\mathbf{x}_0)$.</li>
  <li>Obtain the score for the class of interest, that means the activation before the softmax layer.</li>
  <li>All the rest classes’ activations should be set to zero.</li>
  <li>Back-propagate the gradient of the class of interest to the last convolutional layer before the fully connected layers:</li>
</ul>

\[\frac{\partial y_{c}}{\partial A^k}\]

<ul>
  <li>Weight each feature map “pixel” by the gradient for the class. Indices $i$ and $j$ refer to the width and height dimensions:</li>
</ul>

\[\alpha^{c}_{k} = \overbrace{\frac{1}{Z} \sum_i \sum_j}^{\text{global averaging pooling}} \underbrace{\frac{\partial y_{c}}{\partial A^{k}_{ij}}}_{\text{gradients of the backpropagation}}\]

<p>This means that the gradients are globally pooled.</p>

<ul>
  <li>Calculate an average of the feature maps, weighted per pixel by backpropagated gradient.</li>
  <li>Apply ReLU to the averaged feature map.</li>
</ul>

\[L_{ij}^c = ReLU \sum_k \alpha^{c}_{k} A^{k}_{ij}\]

<p>We now have a heatmap $L^c$ for the class $c$.</p>

<ul>
  <li>Regarding the visualization: Scale values of the $L^c$ to the interval between 0 and 1. Upscale the image and overlay it over the original image.</li>
</ul>

<p>In our classification example this approach uses the activation map of the final convolutional layer (with VGG: the final features layer). Note that such an Activation Map can be a block of $14 \times 14 \times 512$, where the $14 \times 14$ indicated a grid on the image (noted by subscripts i and j) and the 512 is the number of channels (features, noted by the letter k). <strong>Grad Cam</strong> pools the Activation Map over the channels, and it gives a weight equal to the contribution of each channel to the prediction. This contribution of each channel is calculated by taking the gradient of the output w.r.t. the Activation Map and then pool this over the spacial ($14\times14$) dimensions.</p>

<p>For the calculation of the gradient w.r.t the Activation Map we need a little PyTorch trick since this gradient cannot be accessed by default. The PyTorch trick is called a ‘hook’. We can register a hook on a tensor of the network. With a hook we can define a little program that is executed when the tensor is touched during a backward pass. In our case we register a hook on the Activation Map we want to study and that is the 36th layer of the VGG19 convolutional “features” layer. The hook needs to be registered during a forward pass, so we will redefine the forward pass for our model.</p>

<p>There is a nice youtube tutorial on pytorch and hooks https://www.youtube.com/watch?v=syLFCVYua6Q. (22 minutes but I think it is worth it)</p>

<h3 id="define-a-new-vgg-model-including-a-hook">Define a new VGG model including a hook</h3>

<p>The VGG() class is based on the pretrained models.vgg19 that we know now.</p>

<p>In the init, the Activation Map we want to study is defined. That is the output of the first 36 feature layers.</p>

<p>In the activations_hook method we define our hook that will store the gradient calculated on the tensor in self.gradients.</p>

<p>In the forward we execute all VGG layers ‘by hand’. The hook is registered on the output of the first 36 feature layers. And then the remaining layers are defined.</p>

<p>When defined, we load this model, move it to our GPU if available and put the model in eval mode.</p>

<h3 id="activity">Activity:</h3>
<p>Finish the code below by finishing the method get_activations_gradient.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VGG</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># VGG class builds on vgg19 class. The extra feature is the registration of a hook, that
</span>    <span class="c1"># stores the gradient on the last convolutional vgg layer (vgg.features[:36] in self.gradient)
</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">VGG</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="c1"># get the pretrained VGG19 network
</span>        <span class="n">self</span><span class="p">.</span><span class="n">vgg</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">vgg19</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># self.vgg = model
</span>
        <span class="c1"># disect the network to access its last convolutional layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">features_conv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vgg</span><span class="p">.</span><span class="n">features</span><span class="p">[:</span><span class="mi">36</span><span class="p">]</span>
        
        <span class="c1"># get the max pool of the features stem
</span>        <span class="n">self</span><span class="p">.</span><span class="n">max_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        
        <span class="c1"># get the classifier of the vgg19
</span>        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vgg</span><span class="p">.</span><span class="n">classifier</span>
        
        <span class="c1"># placeholder for the gradients
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gradients</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="c1"># hook for the gradients of the activations: it stores the calculated grad (on our tensor) in self.gradients.
</span>    <span class="k">def</span> <span class="nf">activations_hook</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gradients</span> <span class="o">=</span> <span class="n">grad</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># gives the output of the first 36 'feature' layers
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">features_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># register the hook (note: h is a handle, giving the hook a identifier, we do not use it here)
</span>        <span class="n">h</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">register_hook</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">activations_hook</span><span class="p">)</span>

        <span class="c1"># apply the remaining pooling
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">max_pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># apply the remaining classifying
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

    <span class="c1"># method for the gradient extraction
</span>    <span class="k">def</span> <span class="nf">get_activations_gradient</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1">## Should return the gradients of the output with respect to the last convolutional layer
</span>        <span class="c1">## BEGIN Students TODO
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">gradients</span>
        <span class="c1">## END students TODO
</span>    
    <span class="c1"># method for the activation exctraction
</span>    <span class="k">def</span> <span class="nf">get_activations</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">features_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    

<span class="n">vgg</span> <span class="o">=</span> <span class="nc">VGG</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'cuda:'</span><span class="p">,</span> <span class="n">cuda</span><span class="p">,</span> <span class="s">'device:'</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">vgg</span> <span class="o">=</span> <span class="n">vgg</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">vgg</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
</code></pre></div></div>

<p>Now calculate the gradients of a prediction w.r.t. the activation map.**</p>

<p>For that we do a prediction with our newly defined model vgg, and perform a backward on the output (the logit of the prediction vector that is largest). After the backward, the gradients w.r.t the activation map are stored in self.gradient:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get the most likely prediction of the model
</span>
<span class="nb">input</span> <span class="o">=</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>          <span class="c1"># preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
</span><span class="nf">print</span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">pred</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">vgg</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>                         
<span class="nf">print</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>                        

<span class="c1"># also with our newly created VGG(), you should find a correct class (2=shark, 385/386 = elephants)
</span></code></pre></div></div>
<p>And finally:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># pull the gradients out of the model
</span><span class="n">gradients</span> <span class="o">=</span> <span class="n">vgg</span><span class="p">.</span><span class="nf">get_activations_gradient</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'gradients:'</span><span class="p">,</span> <span class="n">gradients</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># pool the gradients across the channels
</span><span class="n">pooled_gradients</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'pooled gradients:'</span><span class="p">,</span> <span class="n">pooled_gradients</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># get the activations of the last convolutional layer
</span><span class="n">activations</span> <span class="o">=</span> <span class="n">vgg</span><span class="p">.</span><span class="nf">get_activations</span><span class="p">(</span><span class="nb">input</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>

<span class="c1"># weight the channels by corresponding gradients
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">512</span><span class="p">):</span>
    <span class="n">activations</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*=</span> <span class="n">pooled_gradients</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
<span class="c1"># average the channels of the activations
</span><span class="n">heatmap</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>

<span class="c1"># relu on top of the heatmap
# expression (2) in https://arxiv.org/pdf/1610.02391.pdf
# heatmap = np.maximum(heatmap, 0)
</span><span class="n">heatmap</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># normalize the heatmap
</span><span class="n">heatmap</span> <span class="o">/=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">heatmap</span><span class="p">)</span>
<span class="c1"># END students TODO
</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'heatmap:'</span><span class="p">,</span> <span class="n">heatmap</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># draw the heatmap
</span><span class="n">heatmap</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">squeeze</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">matshow</span><span class="p">(</span><span class="n">heatmap</span><span class="p">)</span>
</code></pre></div></div>

<font color="green"><b>ToDo 3</b></font>
<p>For your image with 2 animals and each of the two target categories:</p>
<ul>
  <li>Perform the forward pass again, but now with our adapted VGG model</li>
  <li>Draw the Grad-CAM heatmaps $L^c$.</li>
</ul>

<p>Code snippet that might be useful:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gradients, label_index = calculate_outputs_and_gradients([img], vgg, target_label_idx=None, cuda=True)    
gradients = np.transpose(gradients[0], (1, 2, 0))
</code></pre></div></div>

<h3 id="overlaying-heatmaps-and-iamges">Overlaying heatmaps and iamges:</h3>
<p>Now we have the heatmap, we can overlay it on the original image.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># draw the image
</span><span class="nf">print</span><span class="p">(</span><span class="s">'img:'</span><span class="p">,</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">matshow</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">heatmap</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">heatmap</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">heatmap</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">uint8</span><span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">heatmap</span><span class="p">)</span>
<span class="n">heatmap</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">applyColorMap</span><span class="p">(</span><span class="n">heatmap</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLORMAP_JET</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">img</span><span class="p">.</span><span class="nf">min</span><span class="p">(),</span> <span class="n">img</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">heatmap</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">heatmap</span><span class="p">.</span><span class="nf">min</span><span class="p">(),</span> <span class="n">heatmap</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>

<span class="n">super_img</span> <span class="o">=</span> <span class="n">heatmap</span> <span class="o">*</span> <span class="mf">0.4</span> <span class="o">+</span> <span class="n">img</span> <span class="o">*</span> <span class="mf">0.6</span>
<span class="n">super_img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">uint8</span><span class="p">(</span><span class="n">super_img</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">matshow</span><span class="p">(</span><span class="n">super_img</span><span class="p">)</span>
</code></pre></div></div>

<font color="red"><b>ToSubmit 2</b></font>
<p>In your report, include the two Grad-CAM heatmaps for the image with two animals. Add a caption explaining very briefly (1 or 2 sentences) whether there’s a difference and why.</p>

<h3 id="path-integration-methods---integrated-gradients-ig">Path-integration methods - Integrated Gradients (IG)</h3>

<p>As a reminder, the problem that want to study in this tutorial is to find a way to attribute importance in the input features of the vector $\mathbf{x}_i \in \mathbb{R}^{D}$ given the result of the classification from a classifier $\mathcal{F}$.</p>

<p>Suppose that we have a funtion $\mathcal{F}: \mathbb{R}^{D} \to [0, 0, … , 1, … , 0, 0] \in \mathbb{R}^{M} $ which represent a neural network. The input to this network are data $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, …, \mathbf{x}_n) \in \mathbb{R}^{N\times D}$ we would like to calculate a vector $\mathbf{\alpha}_0 = (\alpha_1, \alpha_2, …, \alpha_D) \in \mathbb{R}^{D}$ which is the contribution of the input vector $\mathbf{x}_0$ to the prediction $\mathcal{F}(\mathbf{x}_i)$.</p>

<p>Path-attribution methods in contrast with the gradient methods that we have mentioned before (saliency maps and grad-cam) compare the current image  $\mathbf{x}$ to a reference image $\mathbf{x}^{\prime}$ which can be for instance a black image (or a white image or an image containing random noise). The difference in actual and baseline prediction is divided among the pixels.</p>

<h3 id="ig-approach">IG approach</h3>

<p>As a reminder, the problem that want to study in this tutorial is to find a way to attribute importance in the input features of the vector $\mathbf{x}_i \in \mathbb{R}^{D}$ given the result of the classification from a classifier $\mathcal{F}$.</p>

<p>Suppose that we have a funtion $\mathcal{F}: \mathbb{R}^{D} \to [0, 0, … , 1, … , 0, 0] \in \mathbb{R}^{M} $ which represent a neural network. The input to this network are data $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, …, \mathbf{x}_n) \in \mathbb{R}^{N\times D}$ we would like to calculate a vector $\mathbf{\alpha}_0 = (\alpha_1, \alpha_2, …, \alpha_D) \in \mathbb{R}^{D}$ which is the contribution of the input vector $\mathbf{x}_0$ to the prediction $\mathcal{F}(\mathbf{x}_i)$.</p>

<p>Path-attribution methods in contrast with the gradient methods that we have mentioned before (saliency maps and grad-cam) compare the current image  $\mathbf{x}$ to a reference image $\mathbf{x}^{\prime}$ which can be for instance a black image (or a white image or an image containing random noise). The difference in actual and baseline prediction is divided among the pixels.</p>

<h3 id="calculate-the-integrated-gradients-with-pytorch-recipe">Calculate the integrated gradients with PyTorch recipe.</h3>

<p>Recipe for calculating the IG in our example:</p>
<ul>
  <li><strong>Choose a baseline image</strong>. You can make use of a black/white or an white noise image.</li>
  <li><strong>Build a series of inputs</strong>, each input consist of the baseline plus an additional fraction of the input-image. The final input is the baseline plus the full image. Choose your fraction at 20.</li>
  <li>For each of these inputs, <strong>calculate the gradients of the input</strong> w.r.t. the prediction (using methods under 2 above). Take the average of all these gradients.</li>
  <li><strong>Calculate the difference of image and baseline</strong>: I-B. And calculate Integrated Gradient = (I-B)*average of gradients.</li>
  <li>If you have chosen for another baseline, e.g. for a uniform random generated baseline, then perform this procedure for multiple samples.</li>
</ul>

<h4 id="integrated-gradients-with-one-baseline">Integrated Gradients with one baseline</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># integrated gradients
</span><span class="k">def</span> <span class="nf">integrated_gradients</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">baseline</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c1"># determine baseline
</span>    <span class="k">if</span> <span class="n">baseline</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">baseline</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">*</span> <span class="n">inputs</span> 
    <span class="c1"># scale inputs and compute gradients
</span>    <span class="n">scaled_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">baseline</span> <span class="o">+</span> <span class="p">(</span><span class="nf">float</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="n">steps</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">baseline</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">grads</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">calculate_outputs_and_gradients</span><span class="p">(</span><span class="n">scaled_inputs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>

   <span class="c1"># BEGIN students TODO
</span>    <span class="n">avg_grads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">average</span><span class="p">(</span><span class="n">grads</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>      <span class="c1"># why 51 steps and then remove final result ?
</span>    <span class="n">avg_grads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">avg_grads</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">delta_X</span> <span class="o">=</span> <span class="p">(</span><span class="nf">pre_processing</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span> <span class="o">-</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">baseline</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
    <span class="n">delta_X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">delta_X</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">integrated_grad</span> <span class="o">=</span> <span class="n">delta_X</span> <span class="o">*</span> <span class="n">avg_grads</span>
    <span class="c1"># END students TODO
</span>    <span class="k">return</span> <span class="n">integrated_grad</span>
</code></pre></div></div>

<h4 id="integrated-gradients-with-a-sample-of-random-baselines">Integrated Gradients with a sample of random baselines</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">random_baseline_integrated_gradients</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">num_random_trials</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c1"># when baseline randomly generated, take some samples and average result
</span>    <span class="c1"># BEGIN students TODO
</span>    <span class="n">all_intgrads</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">random_baseline</span> <span class="o">=</span> <span class="mf">255.0</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_random_trials</span><span class="p">):</span>
        <span class="n">integrated_grad</span> <span class="o">=</span> <span class="nf">integrated_gradients</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">baseline</span><span class="o">=</span><span class="n">random_baseline</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span> <span class="n">cuda</span><span class="o">=</span><span class="n">cuda</span><span class="p">)</span>
        <span class="n">all_intgrads</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">integrated_grad</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">'the trial number is: {}'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">avg_intgrads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">average</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">all_intgrads</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># END students TODO
</span>    <span class="k">return</span> <span class="n">avg_intgrads</span>
</code></pre></div></div>

<font color="green"><b>ToDo 4</b></font>
<p>Investigate how well integrated gradients can determine what parts of the image your models is looking at for different target categories. Also investigate whether the zero baseline or a sample of random baselines gives you clearer feature attributions.</p>

<p>Code snippets that might be useful:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># calculate the integrated gradients 
print('img:', img.shape, 'label_index', label_index)

# for zero baseline
int_gradients_zerobl = integrated_gradients(img, model, label_index, baseline=None, steps=50, cuda=cuda)
print('DONE')
# for random baselines, we average over number of trials
int_gradients_randombl = random_baseline_integrated_gradients(img, model, label_index, steps=50, num_random_trials=5, cuda=cuda)    
print('DONE')

# calculate saliency
gradients, _ = calculate_outputs_and_gradients([img], model, None, cuda) 
gradients = np.transpose(gradients[0], (1, 2, 0))

# combine it all in one image
plot_gradients(img, gradients, 'The Image and Its Saliency Map')
plot_gradients(img, int_gradients_zerobl, 'Image and Integrated Gradients with Zero Baseline')
plot_gradients(img, int_gradients_randombl, 'Image and Integrated Gradients with sample of Random Baselines')
</code></pre></div></div>

<font color="red"><b>ToSubmit 3</b></font>

<p>Include in your  Workshop-0 report two images  with results applying  integrated gradients illustrating the strongest differences you have found (i.e., manipulating target categories, baselines, number of samples, or number of steps along the integration path). Include a brief caption that describes the experiment and your interpretation.</p>

<h2 id="lime">LIME</h2>

<p><a href="../../2022/LIME/">LIME tutorial</a></p>

<h2 id="conclusion">Conclusion</h2>

<p>In this post, we have seen two ways of using language for RL. There have been a lot of other ways recently in this direction. Some examples of these are</p>

<ul>
  <li>
    <d-cite key="lampinen-icml22a"></d-cite>
    <p>augment policy networks with the auxiliary target of generating explanations and use this to learn the relational and causal structure of the world</p>
  </li>
  <li>
    <d-cite key="kumar-neurips22a"></d-cite>
    <p>use language to model compositional task distributions and induce human-centric priors into RL agents.</p>
  </li>
</ul>

<p>Given the growth of pre-trained language models, it is only a matter of time before we see many more innovative ideas come around in this field. Language, after all, is a powerful tool to incorporate structural biases into RL pipelines. Additionally, language opens up the possibility of easier interfaces between humans and RL agents, thus, allowing more human-in-the-loop methods to be applied to RL. Finally, the symbolic nature of natural language allows better interpretability in the learned policies, while potentially making them more explainable. Thus, I see this as a very promising direction of future research</p>]]></content><author><name>Christos Athanasiadis</name></author><summary type="html"><![CDATA[This page's main focus is to analyze a branch of explainable & interpretable AI (XAI) called posthoc XAI. We will analyze theory, taxonomy, applications, shortcomings of posthoc XAI approaches and apply them on image classification using popular CNN architectures and explain their black box nature. Part of the assessemnet for this tutorial/workshop, will be some research questions that needs be answered by you. These questions can be found all over this blogspot using the TOSUBMIT tag and will be summarized them at the end of the blogspot.]]></summary></entry><entry><title type="html">Why should I trust you? LIME for image classification</title><link href="https://kristosh.github.io/blog/2022/LIME/" rel="alternate" type="text/html" title="Why should I trust you? LIME for image classification" /><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://kristosh.github.io/blog/2022/LIME</id><content type="html" xml:base="https://kristosh.github.io/blog/2022/LIME/"><![CDATA[<h1 id="local-interpretable-model-agnostic-explanations">Local Interpretable Model-agnostic Explanations</h1>

<center>
<video autoplay="" muted="" loop="" controls="" src="https://kristosh.github.io/assets/video/2022-12-01-LIME/labradors.mp4" style="width:600px" type="video/mp4">
</video>
<figcaption>A demonstration of the whole LIME algorithm.</figcaption>
</center>

<p>In this post, we will study how <mark style="background-color:LavenderBlush;">LIME</mark> (Local Interpretable Model-agnostic Explanations) ([1]) generates explanations for image classification tasks. The basic idea is to understand why a machine learning model predicts that an specific image belongs to a certain class (<em>labrador</em> our visual example). The LIME explainer is <em>model-agnostic</em> that means is not restricted to a specific model and can be used to explain any <mark>black-box</mark> classifier. So we dont need to have access to the details of our model (input, intermediate layers etc) to generate explanations. Moreover, the explainer is <em>local</em> meaning that it explains the prediction of the model in the neighborhood of the instance being explained. This technique lies in the PostHoc category of XAI methods, meaning that it explains the model after it has been trained. Briefly, this technique tried to learn a simple model (for isntance a linear classifier) that is interpretable by humans and approximates the predictions of the <em>black-box</em> model in the neighborhood of the instance being explained.</p>

<h2 id="interpretable-representations">Interpretable Representations</h2>

<p>An interpretable explanation in image classifier should use a representation that is understandable to humans, by explaining which parts of the input image influence the model decision. For example, pixel-based explanations are not very informative especially when we deal with huge images and therefore a better way to explain the model decision is to use <a href="https://infoscience.epfl.ch/record/149300">super-pixels</a>. Super-pixels are groups of pixels that share similar characteristics such as color and texture. Hence, a possible interpretable representation for image classification may be a binary vector indicating the <em>presence</em> or <em>absence</em> of a super-pixel. Thus, our explainer needs to find a way to attribute importance to each super-picel in the initial input image. Its important to note here, that the interpretable representations are meant to be just for the explainer while the <em>black-box</em> can still be trained using the original pixel-based representation.</p>

<p><mark style="background-color:LightCyan;">LIME</mark> approach aims to just give an explanation why the classifier took a specific decision upon a specific input image. It does not aim to explain the whole model. Authors in the paper proposed a mechanism called <mark style="background-color:LightCyan;">SP-LIME</mark> that aims to explain the whole model. While we will not touch this method in this tutorial we encourage you to have a look at it in the original paper.</p>

<h2 id="lime-approach-details">LIME approach details</h2>

<p>To explain how LIME works we will need to introduce some terminology 😎. Hence, let $\mathbf{x} \in R^{d}$ denote the original vector representation of an instance
being explained (in our case a vector with all pixels), and we use $\mathbf{x}^{‘} \in {0, 1}^{d}$ to denote a binary vector for its interpretable representation (super-pixels).</p>

<p>Authors, then define as an explainer (or explanation model) $g \in G$, where $G$ is a class of potentially interpretable models, such as <em>linear models</em>, <em>decision trees</em> etc. To keep things simple, in this tutorial, we will consider just <em>linear classifiers</em>. As not every $g \in G$ may be simple enough to be interpretable thus we let $\Omega(g)$ be a measure of complexity (as opposed to interpretability) of the explanation $g \in G$. For example, for decision trees $\Omega(g)$ may be the depth of the tree, while for linear models, $\Omega(g)$ may be the number of non-zero weights. Authors define as $f : R^{d} \to R$ the <em>black-box</em> model that would like to explain. In classification, $f(\mathbf{x})$ is the probability (or a binary indicator) that $\mathbf{x}$ belongs to a certain class.</p>

<p>They further use $\pi_{\mathbf{x}}(\mathbf{z})$ as a proximity measure between an instance $\mathbf{z}$ to $\mathbf{x}$, so as to define locality around $\mathbf{x}$. Finally, let $\mathcal{L}(f, g, \pi_{\mathbf{x}})$ be a measure of how unfaithful $g$ is in approximating $f$ in the locality defined by $\pi_{\mathbf{x}}$. In order to ensure both <em>interpretability</em> and <em>local fidelity</em>, they must minimize $L(f, g, \pi_{x})$ while having $\Omega(g)$ be low enough to be interpretable by humans.</p>

<p>The explanation produced by LIME is obtained by the following:</p>

\[\xi(\mathbf{x}) = \mathcal{L}(f, g, \pi_{\mathbf{x}}) + \Omega(g)\]

<p>The above equation contains the tradeoff between <en>local fidelity&lt;/em&gt; that it is extrpressed by $L$ and <em>complexity</em> that it is expressed by $\Omega$.</en></p>

<p>The first tem $\mathcal{L}(f, g, \pi_{\mathbf{x}})$ in the paper is represented by the weighted square loss:</p>

\[\mathcal{L}(f, g, \pi_{\mathbf{x}}) = \sum_{\mathbf{z}, \mathbf{z}^{'}}\pi_{\mathbf{x}}(\mathbf{z})(f(\mathbf{z})- g(\mathbf{z}^{'}))^{2}\]

<p>with $\pi_{\mathbf{x}}$ to be a kernel function that measures the proximity of $z$ to $x$:</p>

\[\pi_{\mathbf{x}} =  \exp(-D(\mathbf{x},\mathbf{z})^{2}/\sigma*{2})\]

<p>The idea is that tuning the weights $\mathbf{w}$ we can use them directly as a feature attribution to each super-pixel. The higher the weight that corresponds to a specific super-pixel the more important this super-pixel is for the prediction of the <em>black-box</em> model and the vice-versa.</p>

<p>Another importance explanation is about the terms <em>faithfullness</em> and <em>local-fidelity</em> and they about how well our explainer $g$ can approximate the decision of the <em>black-box</em> model $f$ in the locality defined by $\pi_{\mathbf{x}}$.</p>

<p>The whole LIME algorithm can be summarized as follows:</p>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2022-12-01-LIME/algorithm.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>The kernel is the distance between the instance being explained and the instance in the neighborhood. The neighborhood is generated by <mark style="background-color:LightCyan;">sampling</mark> instances around the instance being explained. The sampling is done by perturbing the instance being explained. For example, in the case of images, we can perturb the image by zeroing out some super-pixels. The perturbed instances are then fed to the <em>black-box</em> model and the output is used to train the explainer. The weights of the interpretable model are then used to explain the prediction of the <em>black-box</em> model. Finally, in the algorithm the <mark style="background-color:Lavender;">K-lasso</mark> refers to the regulizarion that is introduced in a previous equation and relates with the term $\Omega(g)$.</p>

<center>
<video autoplay="" muted="" loop="" controls="" src="https://kristosh.github.io/assets/video/2022-12-01-LIME/LIME.mp4" style="width:600px" type="video/mp4">
</video>
<figcaption>A demonstration of the whole LIME algorithm.</figcaption>
</center>

<p>The above video explain the whole LIME process. The initial surface represents the <mark style="background-color:Lavender;">black-box</mark> classifier and the regions for the class of interest (e.g. basketball with the light-pink color). The dark-colored dot denotes the sample that we would like to explain and it is actually an image with the label <em> basketball</em>. The first step is to sample the neighborhood of the point $\mathbf{x}$ that we would like to explain. Several points are generated. The size of each generated sample and the trasparency relates with the distance from the initial point $\mathbf{x}$ which is calculated based using $\pi_{\mathbf{x}}(\mathbf{z})$. The next step is to apply the <mark style="background-color:Lavender;">black-box</mark> classifier $f()$ to find the label for each generated point. Samples with red represetns class basketball while samples with purple represents class not basketball. The next step is to train the interpretable model $g()$ using the generated samples. The weights of the interpretable model are used to explain the prediction of the <mark style="background-color:Lavender;">black-box</mark> classifier. The explanation is actually a linear classifier that separates the two classes. The weights of the linear classifier can be used as an explanation for the whole approach.</p>

<h2 id="code-implementation">Code implementation</h2>

<p>Firstly, we will need to import the required libraries. The code is written in Python 3.6.9 and PyTorch 1.7.0. The code is available in the following <a href="TBA">link</a></p>
<h3 id="imports">Imports</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">os</span><span class="p">,</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'KMP_DUPLICATE_LIB_OK'</span><span class="p">]</span><span class="o">=</span><span class="s">'True'</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">models</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="n">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">copy</span>

<span class="kn">import</span> <span class="n">sklearn</span>
<span class="kn">import</span> <span class="n">sklearn.metrics</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
</code></pre></div></div>

<h3 id="initialization-of-a-vgg19-model">Initialization of a VGG19 model</h3>
<p>A pre-trained VGG19 model is used to predict the class of the image. The output of the classification is a vector of 1000 proabilities of beloging to each class available in VGG19. The model is initialized and the weights are loaded. The model is set to evaluation mode. The model is set to run on GPU if available.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load model
# model_type = 'vgg19'
</span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">vgg19</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># run it on a GPU if available:
</span><span class="n">cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'cuda:'</span><span class="p">,</span> <span class="n">cuda</span><span class="p">,</span> <span class="s">'device:'</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># set model to evaluation
</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
</code></pre></div></div>
<p>Ignore for now the warnings! This code should return the architecture of the VGG19 model. Of course, feel-free to choose the model of you choice the code should work with any model.
Now lets load and process the image (for the VGG19 classifier) that we would like to test our <mark style="background-color:Lavender;">LIME</mark> explainer. You can freely choose your own image that you would like to explain.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">imread_img</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
  
  <span class="c1"># read the image and convert it - Set your pathto the image
</span>  <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="s">'caretta.png'</span><span class="p">)</span>
  <span class="nf">if </span><span class="p">(</span><span class="nf">type</span><span class="p">(</span><span class="n">img</span><span class="p">)</span> <span class="ow">is</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">'img:'</span><span class="p">,</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">'image not found - set your path to the image'</span><span class="p">)</span>
  
  <span class="k">return</span> <span class="n">img</span>
</code></pre></div></div>

<h3 id="image-pre-processing">Image pre-processing</h3>

<p>As usual, we will need to normalize our input image. The normalization is done using the mean and standard deviation of the ImageNet dataset. The image is also transposed to the correct tensor format:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">cuda</span><span class="p">):</span>
    <span class="c1"># Students should transpose the image to the correct tensor format. 
</span>    <span class="c1"># Students should ensure that gradient for input is calculated       
</span>    <span class="c1"># set the GPU device
</span>    <span class="k">if</span> <span class="n">cuda</span><span class="p">:</span>
        <span class="n">torch_device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">'cuda:0'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">torch_device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>

    <span class="c1"># normalise for ImageNet
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span> <span class="o">/</span> <span class="mi">255</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="p">(</span><span class="n">obs</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>

    <span class="c1"># make tensor format that keeps track of gradient
</span>    <span class="c1"># BEGIN for students to do
</span>    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>       
    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="n">obs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch_device</span><span class="p">)</span>
    <span class="c1"># END for students to do
</span>    <span class="k">return</span> <span class="n">obs_tensor</span>
</code></pre></div></div>

<p>Then, we check the prediction by using the VGG19 model that we loaded before. You can check that the VGG network gives a correct prediction. E.g. 33 and 34 are ‘caretta-caretta’and ‘turtle’</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">cuda</span><span class="p">):</span>
    <span class="c1"># Makes prediction after preprocessing image 
</span>    <span class="c1"># Note that output should be torch.tensor on cuda
</span>    <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>                        
    <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># calc output from model 
</span>    <span class="k">if</span> <span class="n">target_label_idx</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">target_label_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">output</span><span class="p">.</span><span class="nf">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">target_label_idx</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span> 
    <span class="k">if</span> <span class="n">cuda</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>                     <span class="c1"># calc prediction
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>           <span class="c1"># gather functionality of pytorch
</span>    <span class="k">return</span> <span class="n">target_label_idx</span><span class="p">,</span> <span class="n">output</span> 

<span class="nb">input</span> <span class="o">=</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>          <span class="c1"># preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
</span><span class="nf">print </span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">label</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'output:'</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'output label:'</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># resize and take the center part of image to what our model expects
</span><span class="k">def</span> <span class="nf">get_input_transform</span><span class="p">():</span>
    <span class="n">normalize</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                    <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>       
    <span class="n">transf</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
        <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
        <span class="n">normalize</span>
    <span class="p">])</span>    

    <span class="k">return</span> <span class="n">transf</span>

<span class="k">def</span> <span class="nf">get_input_tensors</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">transf</span> <span class="o">=</span> <span class="nf">get_input_transform</span><span class="p">()</span>
    <span class="c1"># unsqeeze converts single image to batch of 1
</span>    <span class="k">return</span> <span class="nf">transf</span><span class="p">(</span><span class="n">img</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>The following code helps you to get the label of the prediction. The label is the index of the class in the ImageNet dataset. The index is used to get the class name from the json file. The json file is available in the repository.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idx2label</span><span class="p">,</span> <span class="n">cls2label</span><span class="p">,</span> <span class="n">cls2idx</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">{},</span> <span class="p">{}</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">abspath</span><span class="p">(</span><span class="s">'imagenet_class_index.json'</span><span class="p">),</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">read_file</span><span class="p">:</span>
    <span class="n">class_idx</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">read_file</span><span class="p">)</span>
    <span class="n">idx2label</span> <span class="o">=</span> <span class="p">[</span><span class="n">class_idx</span><span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">k</span><span class="p">)][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">class_idx</span><span class="p">))]</span>
    <span class="n">cls2label</span> <span class="o">=</span> <span class="p">{</span><span class="n">class_idx</span><span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">k</span><span class="p">)][</span><span class="mi">0</span><span class="p">]:</span> <span class="n">class_idx</span><span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">k</span><span class="p">)][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">class_idx</span><span class="p">))}</span>
    <span class="n">cls2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">class_idx</span><span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">k</span><span class="p">)][</span><span class="mi">0</span><span class="p">]:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">class_idx</span><span class="p">))}</span> 
</code></pre></div></div>

<h2 id="lime-explanation">LIME explanation</h2>
<p>The following figure illustrates the basic idea behind LIME. The figure shows with light pink and blue areas which are the decision boundaries for the classifier (for the VGG19 pre-trained model on ImageNet for instance). <mark style="background-color:Lavender;">LIME</mark> is able to provide explanations for the predictions of an individual instance (the one with the dark red dot). These explanations are created by generating a new dataset of perturbations around the instance to be explained (in our image are depicted with dot circles around the initial instance).</p>

<p>Then, we apply our <mark style="background-color:Lavender;">black-box</mark> model $g()$ and we can extract the label for all the pertubations (and can be seen which red that denotes turtle and purple that denotes non-turtle). The <em>importance</em> of each perturbation is determined by measuring its distance from the original instance to be explained. These distances are converted to weights by mapping the distances to a zero-one scale using a kernel function (see color scale for the weights). All this information: the new generated dataset, its class predictions and its weights are used to fit a simpler model, such as a linear model (gray line), that can be interpreted. The coefficients for the case of a linear model, are then extracted and used as the explanation for the prediction of the instance to be explained. The higher the coefficient, the more important the feature is for the prediction.</p>

<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2022-12-01-LIME/LIME.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h2 id="creating-perturbations-of-image">Creating Perturbations of image</h2>

<p>For the case of image explanations, perturbations will be generated by turning on and off some of the superpixels in the image.</p>

<h4 id="extract-super-pixels-from-image">Extract super-pixels from image</h4>
<p>Superpixels are generated using the quickshift segmentation algorithm. It can be noted that for the given image, <mark style="background-color:Lavender;">68</mark> superpixels were generated. That of course will change if you will update the <mark style="background-color:Lavender;">quickshift</mark> parameters (<mark style="background-color:Lavender;">kernel_size</mark> and <mark style="background-color:Lavender;">max_dist</mark>). The generated superpixels are shown in the image below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">skimage.io</span> 
<span class="kn">import</span> <span class="n">skimage.segmentation</span>

<span class="n">superpixels</span> <span class="o">=</span> <span class="n">skimage</span><span class="p">.</span><span class="n">segmentation</span><span class="p">.</span><span class="nf">quickshift</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">max_dist</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">ratio</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">num_superpixels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">superpixels</span><span class="p">).</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">num_superpixels</span>

<span class="n">imgplot</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">skimage</span><span class="p">.</span><span class="n">segmentation</span><span class="p">.</span><span class="nf">mark_boundaries</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span> <span class="n">superpixels</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>
<p><img src="../../../../assets/img/2022-12-01-LIME/caretta_2.jpg" style="width:400px;height:400px;" class="center" /></p>

<h3 id="creating-random-perturbations">Creating random perturbations</h3>
<p>In this example, 150 perturbations were used. However, for real life applications, a larger number of perturbations will produce more reliable explanations. Random zeros and ones are generated and shaped as a matrix with perturbations as rows and superpixels as columns. An example of a perturbation (the first one) is show below. Here, <code class="language-plaintext highlighter-rouge">1</code> represent that a superpixel is on and <code class="language-plaintext highlighter-rouge">0</code> represents it is off. Notice that the length of the shown vector corresponds to the number of superpixels in the image.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_perturb</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">perturbations</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_perturb</span><span class="p">,</span> <span class="n">num_superpixels</span><span class="p">))</span>
<span class="n">perturbations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#Show example of perturbation
</span>
</code></pre></div></div>
<p>The following function <code class="language-plaintext highlighter-rouge">perturb_image</code> perturbs the given image (<code class="language-plaintext highlighter-rouge">img</code>) based on a perturbation vector (<code class="language-plaintext highlighter-rouge">perturbation</code>) and predefined superpixels (<code class="language-plaintext highlighter-rouge">segments</code>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">perturb_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">perturbation</span><span class="p">,</span><span class="n">segments</span><span class="p">):</span>
  <span class="n">active_pixels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">perturbation</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">segments</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">active</span> <span class="ow">in</span> <span class="n">active_pixels</span><span class="p">:</span>
      <span class="n">mask</span><span class="p">[</span><span class="n">segments</span> <span class="o">==</span> <span class="n">active</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> 
  <span class="n">perturbed_image</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
  <span class="n">perturbed_image</span> <span class="o">=</span> <span class="n">perturbed_image</span><span class="o">*</span><span class="n">mask</span><span class="p">[:,:,</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">perturbed_image</span>
</code></pre></div></div>

<p>Let’s use the previous function to see what a perturbed image would look like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">skimage</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="nf">perturb_image</span><span class="p">(</span><span class="n">img</span><span class="o">/</span><span class="mi">2</span><span class="o">+</span><span class="mf">0.5</span><span class="p">,</span><span class="n">perturbations</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">superpixels</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="../../../../assets/img/2022-12-01-LIME/caretta_3.jpg" style="width:400px;height:400px;" class="center" /></p>

<h3 id="step-2-use-ml-classifier-to-predict-classes-of-new-generated-images">Step 2: Use ML classifier to predict classes of new generated images</h3>
<p>This is the most computationally expensive step in LIME because a prediction for each perturbed image is computed. From the shape of the predictions we can see for each of the perturbations we have the output probability for each of the 1000 classes in Inception V3.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">pert</span> <span class="ow">in</span> <span class="n">perturbations</span><span class="p">:</span>
  <span class="n">perturbed_img</span> <span class="o">=</span> <span class="nf">perturb_image</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">pert</span><span class="p">,</span><span class="n">superpixels</span><span class="p">)</span>
  <span class="nb">input</span> <span class="o">=</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">perturbed_img</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>   
  <span class="c1"># preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
</span>  <span class="n">output</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>
  
  <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>                        
  <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="nf">print </span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="n">target_label_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>
  
  <span class="n">predictions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">predictions</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">original_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">num_superpixels</span><span class="p">)[</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,:]</span> <span class="c1">#Perturbation with all superpixels enabled 
</span><span class="n">distances</span> <span class="o">=</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="nf">pairwise_distances</span><span class="p">(</span><span class="n">perturbations</span><span class="p">,</span><span class="n">original_image</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s">'cosine'</span><span class="p">).</span><span class="nf">ravel</span><span class="p">()</span>
<span class="n">distances</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kernel_width</span> <span class="o">=</span> <span class="mf">0.25</span>	
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">distances</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">kernel_width</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="c1">#Kernel function 
</span><span class="n">weights</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">img</span> <span class="o">=</span> <span class="nf">imread_img</span><span class="p">(</span><span class="s">'caretta.jpg'</span><span class="p">)</span>

<span class="nb">input</span> <span class="o">=</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cuda</span><span class="p">)</span>          <span class="c1"># preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
</span>
<span class="n">out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>     
<span class="nf">print </span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>                
<span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>    

<span class="n">out</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">top_values</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">5</span><span class="p">]</span> <span class="c1"># Keep the first 5 values from each row
</span><span class="n">top_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">5</span><span class="p">]</span>   <span class="c1"># Keep the corresponding indices
</span>
<span class="n">top5</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">topk_values</span> <span class="o">=</span> <span class="n">top_values</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">topk_indices</span> <span class="o">=</span>  <span class="n">top_indices</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">topk_values</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">topk_indices</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">simpler_model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>
<span class="c1"># print (topk_indices[0][0])
# print (perturbations.shape)
# print (predictions[:,:,topk_indices[0][0]])
</span><span class="n">simpler_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">perturbations</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">predictions</span><span class="p">[:,:,</span><span class="n">topk_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">simpler_model</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">coeff</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_top_features</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">top_features</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">coeff</span><span class="p">)[</span><span class="o">-</span><span class="n">num_top_features</span><span class="p">:]</span> 
<span class="n">top_features</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">num_superpixels</span><span class="p">)</span> 
<span class="n">mask</span><span class="p">[</span><span class="n">top_features</span><span class="p">]</span><span class="o">=</span> <span class="bp">True</span> <span class="c1">#Activate top superpixels
</span>
<span class="n">img</span> <span class="o">=</span> <span class="nf">imread_img</span><span class="p">(</span><span class="s">'caretta.jpg'</span><span class="p">)</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">/</span><span class="mi">255</span>
<span class="n">skimage</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="nf">perturb_image</span><span class="p">(</span><span class="n">img</span> <span class="p">,</span><span class="n">mask</span><span class="p">,</span><span class="n">superpixels</span><span class="p">)</span> <span class="p">)</span>
</code></pre></div></div>

<p><img src="../../../../assets/img/2022-12-01-LIME/caretta_4.jpg" style="width:400px;height:400px;" class="center" /></p>

<h1 id="results">Results</h1>

<h1 id="you-should-not-trust-me">You should not trust me</h1>

<h1 id="tosubmit">TOSUBMIT</h1>

<h1 id="conclusions">Conclusions</h1>

<h1 id="references">References</h1>
<p style="font-size: smaller"><a href="https://arxiv.org/pdf/1602.04938.pdf">[1] Ribeuro, M.T. et al. Why Should I Trust You? Explaining the Predictions of Any Classifier, 2016. SIGKDD.</a></p>

<p style="font-size: smaller"><a href="https://arxiv.org/pdf/2205.11487">[2] Saharia, C. et al. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, 2022. <em>arXiv Preprint</em>.</a></p>

<p style="font-size: smaller"><a href="https://arxiv.org/pdf/2209.14792">[3] Singer, U. et al. Make-a-Video: Text-to-Video Generation Without Text-Video Data, 2022. <em>arXiv Preprint</em>.</a></p>

<p style="font-size: smaller"><a href="https://arxiv.org/pdf/2210.02303">[4] Ho, J. et al. Imagen Video: High Definition Video Generation with Diffusion Models, 2022. <em>arXiv Preprint</em>.</a></p>]]></content><author><name>Christos Athanasiadis</name></author><summary type="html"><![CDATA[This page's goal is to present a PostHoc feature attribution XAI methodology called LIME (Local Interpretable Model-agnostic Explanations) and how it can be used to explain image classification tasks. You will be guided through the code and the results of the LIME algorithm. Part of the assessemnet for this workshop, will be some research questions that needs be answered by you. These questions can be found all over this blogspot using the TOSUBMIT tag and will be summarized them at the end of the blogspot.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://kristosh.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog" /><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://kristosh.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://kristosh.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">a post with diagrams</title><link href="https://kristosh.github.io/blog/2021/diagrams/" rel="alternate" type="text/html" title="a post with diagrams" /><published>2021-07-04T17:39:00+00:00</published><updated>2021-07-04T17:39:00+00:00</updated><id>https://kristosh.github.io/blog/2021/diagrams</id><content type="html" xml:base="https://kristosh.github.io/blog/2021/diagrams/"><![CDATA[<p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin.
Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p>

<p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine.
Also, be mindful of that because of diagram generation the fist time you build your Jekyll website after adding new diagrams will be SLOW.
For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p>

<h2 id="mermaid">Mermaid</h2>

<p>Install mermaid using <code class="language-plaintext highlighter-rouge">node.js</code> package manager <code class="language-plaintext highlighter-rouge">npm</code> by running the following command:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>npm <span class="nb">install</span> <span class="nt">-g</span> mermaid.cli
</code></pre></div></div>

<p>The diagram below was generated by the following code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div>

<div class="jekyll-diagrams diagrams mermaid">
  <svg id="mermaid-1683903200333" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1683903200333 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1683903200333 .node circle,#mermaid-1683903200333 .node ellipse,#mermaid-1683903200333 .node polygon,#mermaid-1683903200333 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1683903200333 .node.clickable{cursor:pointer}#mermaid-1683903200333 .arrowheadPath{fill:#333}#mermaid-1683903200333 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1683903200333 .edgeLabel{background-color:#e8e8e8}#mermaid-1683903200333 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1683903200333 .cluster text{fill:#333}#mermaid-1683903200333 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1683903200333 .actor{stroke:#ccf;fill:#ececff}#mermaid-1683903200333 text.actor{fill:#000;stroke:none}#mermaid-1683903200333 .actor-line{stroke:grey}#mermaid-1683903200333 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1683903200333 .messageLine0,#mermaid-1683903200333 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1683903200333 #arrowhead{fill:#333}#mermaid-1683903200333 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1683903200333 .messageText{fill:#333;stroke:none}#mermaid-1683903200333 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1683903200333 .labelText,#mermaid-1683903200333 .loopText{fill:#000;stroke:none}#mermaid-1683903200333 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1683903200333 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1683903200333 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1683903200333 .section{stroke:none;opacity:.2}#mermaid-1683903200333 .section0{fill:rgba(102,102,255,.49)}#mermaid-1683903200333 .section2{fill:#fff400}#mermaid-1683903200333 .section1,#mermaid-1683903200333 .section3{fill:#fff;opacity:.2}#mermaid-1683903200333 .sectionTitle0,#mermaid-1683903200333 .sectionTitle1,#mermaid-1683903200333 .sectionTitle2,#mermaid-1683903200333 .sectionTitle3{fill:#333}#mermaid-1683903200333 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1683903200333 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1683903200333 .grid path{stroke-width:0}#mermaid-1683903200333 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1683903200333 .task{stroke-width:2}#mermaid-1683903200333 .taskText{text-anchor:middle;font-size:11px}#mermaid-1683903200333 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1683903200333 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1683903200333 .taskText0,#mermaid-1683903200333 .taskText1,#mermaid-1683903200333 .taskText2,#mermaid-1683903200333 .taskText3{fill:#fff}#mermaid-1683903200333 .task0,#mermaid-1683903200333 .task1,#mermaid-1683903200333 .task2,#mermaid-1683903200333 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1683903200333 .taskTextOutside0,#mermaid-1683903200333 .taskTextOutside1,#mermaid-1683903200333 .taskTextOutside2,#mermaid-1683903200333 .taskTextOutside3{fill:#000}#mermaid-1683903200333 .active0,#mermaid-1683903200333 .active1,#mermaid-1683903200333 .active2,#mermaid-1683903200333 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1683903200333 .activeText0,#mermaid-1683903200333 .activeText1,#mermaid-1683903200333 .activeText2,#mermaid-1683903200333 .activeText3{fill:#000!important}#mermaid-1683903200333 .done0,#mermaid-1683903200333 .done1,#mermaid-1683903200333 .done2,#mermaid-1683903200333 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1683903200333 .doneText0,#mermaid-1683903200333 .doneText1,#mermaid-1683903200333 .doneText2,#mermaid-1683903200333 .doneText3{fill:#000!important}#mermaid-1683903200333 .crit0,#mermaid-1683903200333 .crit1,#mermaid-1683903200333 .crit2,#mermaid-1683903200333 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1683903200333 .activeCrit0,#mermaid-1683903200333 .activeCrit1,#mermaid-1683903200333 .activeCrit2,#mermaid-1683903200333 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1683903200333 .doneCrit0,#mermaid-1683903200333 .doneCrit1,#mermaid-1683903200333 .doneCrit2,#mermaid-1683903200333 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1683903200333 .activeCritText0,#mermaid-1683903200333 .activeCritText1,#mermaid-1683903200333 .activeCritText2,#mermaid-1683903200333 .activeCritText3,#mermaid-1683903200333 .doneCritText0,#mermaid-1683903200333 .doneCritText1,#mermaid-1683903200333 .doneCritText2,#mermaid-1683903200333 .doneCritText3{fill:#000!important}#mermaid-1683903200333 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1683903200333 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1683903200333 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1683903200333 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1683903200333 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1683903200333 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1683903200333 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1683903200333 #compositionEnd,#mermaid-1683903200333 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1683903200333 #aggregationEnd,#mermaid-1683903200333 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1683903200333 #dependencyEnd,#mermaid-1683903200333 #dependencyStart,#mermaid-1683903200333 #extensionEnd,#mermaid-1683903200333 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1683903200333 .branch-label,#mermaid-1683903200333 .commit-id,#mermaid-1683903200333 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1683903200333 {
    color: rgb(0, 0, 0);
    font: normal normal 400 normal 16px / normal "Times New Roman";
  }</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[an example of a blog post with diagrams]]></summary></entry><entry><title type="html">a distill-style blog post</title><link href="https://kristosh.github.io/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post" /><published>2021-05-22T00:00:00+00:00</published><updated>2021-05-22T00:00:00+00:00</updated><id>https://kristosh.github.io/blog/2021/distill</id><content type="html" xml:base="https://kristosh.github.io/blog/2021/distill/"><![CDATA[<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<hr />

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

<hr />

<h2 id="code-blocks">Code Blocks</h2>

<p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags.
An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>.
For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p>

<d-code block="" language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

<p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode.
You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="nx">x</span> <span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<hr />

<h2 id="interactive-plots">Interactive Plots</h2>

<p>You can add interative plots using plotly + iframes :framed_picture:</p>

<div class="l-page">
  <iframe src="/assets/plotly/demo.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<p>The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span>
  <span class="s">'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'</span>
<span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
  <span class="n">df</span><span class="p">,</span>
  <span class="n">lat</span><span class="o">=</span><span class="s">'Latitude'</span><span class="p">,</span>
  <span class="n">lon</span><span class="o">=</span><span class="s">'Longitude'</span><span class="p">,</span>
  <span class="n">z</span><span class="o">=</span><span class="s">'Magnitude'</span><span class="p">,</span>
  <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
  <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span>
  <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
  <span class="n">mapbox_style</span><span class="o">=</span><span class="s">"stamen-terrain"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="s">'assets/plotly/demo.html'</span><span class="p">)</span></code></pre></figure>

<hr />

<h2 id="details-boxes">Details boxes</h2>

<p>Details boxes are collapsible boxes which hide additional information from the user. They can be added with the <code class="language-plaintext highlighter-rouge">details</code> liquid tag:</p>

<details><summary>Click here to know more</summary>
<p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p>
</details>

<hr />

<h2 id="layouts">Layouts</h2>

<p>The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p>

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

<p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p>

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

<p>All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:</p>

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

<p>Occasionally you’ll want to use the full browser width.
For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>.
You can also inset the element a little from the edge of the browser by using the inset variant.</p>

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

<p>The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p>

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

<hr />

<h2 id="other-typography">Other Typography?</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>

<ol>
  <li>First ordered list item</li>
  <li>Another item
⋅⋅* Unordered sub-list.</li>
  <li>Actual numbers don’t matter, just that it’s a number
⋅⋅1. Ordered sub-list</li>
  <li>And another item.</li>
</ol>

<p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p>

<p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p>

<ul>
  <li>Unordered list can use asterisks</li>
  <li>Or minuses</li>
  <li>Or pluses</li>
</ul>

<p><a href="https://www.google.com">I’m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I’m a reference-style link</a></p>

<p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links.
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here’s our logo (hover to see the title text):</p>

<p>Inline-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1" /></p>

<p>Reference-style:
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2" /></p>

<p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="s">"Python syntax highlighting"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div>

<p>Colons can be used to align columns.</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the
raw Markdown line up prettily. You can also use inline Markdown.</p>

<table>
  <thead>
    <tr>
      <th>Markdown</th>
      <th>Less</th>
      <th>Pretty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Still</em></td>
      <td><code class="language-plaintext highlighter-rouge">renders</code></td>
      <td><strong>nicely</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
  <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p>
</blockquote>

<p>Here’s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry><entry><title type="html">a post with github metadata</title><link href="https://kristosh.github.io/blog/2020/github-metadata/" rel="alternate" type="text/html" title="a post with github metadata" /><published>2020-09-28T21:01:00+00:00</published><updated>2020-09-28T21:01:00+00:00</updated><id>https://kristosh.github.io/blog/2020/github-metadata</id><content type="html" xml:base="https://kristosh.github.io/blog/2020/github-metadata/"><![CDATA[<p>A sample blog page that demonstrates the accessing of github meta data.</p>

<h2 id="what-does-github-metadata-do">What does Github-MetaData do?</h2>
<ul>
  <li>Propagates the site.github namespace with repository metadata</li>
  <li>Setting site variables :
    <ul>
      <li>site.title</li>
      <li>site.description</li>
      <li>site.url</li>
      <li>site.baseurl</li>
    </ul>
  </li>
  <li>Accessing the metadata - duh.</li>
  <li>Generating edittable links.</li>
</ul>

<h2 id="additional-reading">Additional Reading</h2>
<ul>
  <li>If you’re recieving incorrect/missing data, you may need to perform a Github API<a href="https://github.com/jekyll/github-metadata/blob/master/docs/authentication.md"> authentication</a>.</li>
  <li>Go through this <a href="https://jekyll.github.io/github-metadata/">README</a> for more details on the topic.</li>
  <li><a href="https://github.com/jekyll/github-metadata/blob/master/docs/site.github.md">This page</a> highlights all the feilds you can access with github-metadata.
<br /></li>
</ul>

<h2 id="example-metadata">Example MetaData</h2>
<ul>
  <li>Host Name :</li>
  <li>URL :</li>
  <li>BaseURL :</li>
  <li>Archived :</li>
  <li>Contributors :</li>
</ul>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><summary type="html"><![CDATA[a quick run down on accessing github metadata.]]></summary></entry><entry><title type="html">a post with twitter</title><link href="https://kristosh.github.io/blog/2020/twitter/" rel="alternate" type="text/html" title="a post with twitter" /><published>2020-09-28T15:12:00+00:00</published><updated>2020-09-28T15:12:00+00:00</updated><id>https://kristosh.github.io/blog/2020/twitter</id><content type="html" xml:base="https://kristosh.github.io/blog/2020/twitter/"><![CDATA[<p>A sample blog page that demonstrates the inclusion of Tweets/Timelines/etc.</p>

<h1 id="tweet">Tweet</h1>
<p>An example of displaying a tweet:</p>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<h1 id="timeline">Timeline</h1>
<p>An example of pulling from a timeline:</p>
<div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<h1 id="additional-details">Additional Details</h1>
<p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><category term="formatting" /><summary type="html"><![CDATA[an example of a blog post with twitter]]></summary></entry><entry><title type="html">a post with code</title><link href="https://kristosh.github.io/blog/2015/code/" rel="alternate" type="text/html" title="a post with code" /><published>2015-07-15T15:09:00+00:00</published><updated>2015-07-15T15:09:00+00:00</updated><id>https://kristosh.github.io/blog/2015/code</id><content type="html" xml:base="https://kristosh.github.io/blog/2015/code/"><![CDATA[<p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting.
It supports more than 100 languages.
This example is in C++.
All you have to do is wrap your code in markdown code tags:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">c++
</span><span class="n">code</span> <span class="n">code</span> <span class="n">code</span>
<span class="p">```</span>
</code></pre></div></div>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
    <span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>By default, it does not display line numbers. If you want to display line numbers for every code block, you can set <code class="language-plaintext highlighter-rouge">kramdown.syntax_highlighter_opts.block.line_numbers</code> to true in your <code class="language-plaintext highlighter-rouge">_config.yml</code> file.</p>

<p>If you want to display line numbers for a specific code block, all you have to do is wrap your code in a liquid tag:</p>

<p>{% highlight c++ linenos %}  <br /> code code code <br /> {% endhighlight %}</p>

<p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers.
Produces something like this:</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
    <span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="sample-posts" /><category term="formatting" /><category term="code" /><summary type="html"><![CDATA[an example of a blog post with some code]]></summary></entry><entry><title type="html">a post with images</title><link href="https://kristosh.github.io/blog/2015/images/" rel="alternate" type="text/html" title="a post with images" /><published>2015-05-15T21:01:00+00:00</published><updated>2015-05-15T21:01:00+00:00</updated><id>https://kristosh.github.io/blog/2015/images</id><content type="html" xml:base="https://kristosh.github.io/blog/2015/images/"><![CDATA[<p>This is an example post with image galleries.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

<p>Images can be made zoomable.
Simply add <code class="language-plaintext highlighter-rouge">data-zoomable</code> to <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags that you want to make zoomable.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>The rest of the images in this post are all zoomable, arranged into different mini-galleries.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/11.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>]]></content><author><name></name></author><category term="sample-posts" /><category term="formatting" /><category term="images" /><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry></feed>