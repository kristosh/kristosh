---
layout: distill
title: Local Interpretable Model-agnostic Explanations
description: This page's goal is to present an feature attribution XAI methodology called LIME (Local Interpretable Model-agnostic Explanations) and how it can be used to explain image classification tasks. You will guided through the code and the results of the LIME algorithm. Part of the assessemnet for this tutorial/workshop, will be some research questions that needs be answered by you. These questions can be found all over this blogspot using the TOSUBMIT tag and will be summarized them at the end of the blogspot.
date: 2022-12-01
htmlwidgets: true

# anonymize when submitting
authors:
  - name: Anonymous

# do not fill this in until your post is accepted and you're publishing your camera-ready post!
authors:
  - name: Christos Athanasiadis
    url: "https://www.linkedin.com/in/christos-athanasiadis-a3b51035/"
    affiliations:
      name: UvA, Interpretability and Explainability in AI

# must be the exact same name as your blogpost
# bibliography: 2022-12-01-distill-example.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Local Interpretable Model-agnostic Explanations
  - name: History of Text-to-Video
  - name: Text-to-Image Generation
    subsections:
    - name: What is latent space?
    - name: How does stable diffusion work in latent space?
  - name: Text-to-Video Generation
    subsections:
    - name: How do we extend Text-to-Image to Text-to-Video?
    - name: Spatial and Temporal Super Resolution
  - name: Conclusions
    subsections:
    - name: Putting Together All The Building Blocks
    - name: Limitations of Text-to-Video
  - name: Related Works
  - name: References
---

# Local Interpretable Model-agnostic Explanations



![banner](https://arteagac.github.io/blog/lime_image/img/lime_banner.png)

In this post, we will study how LIME  (Local Interpretable Model-agnostic Explanations) ([Ribeiro et. al. 2016](https://arxiv.org/abs/1602.04938)) generates explanations for image classification tasks. The basic idea is to understand why a machine learning model (deep neural network) predicts that an instance (image) belongs to a certain class (labrador in this case). Also, the following YouTube video explains this notebook step by step.


<iframe src="https://www.youtube.com/embed/ENa-w65P1xM" width="560" height="315"  allowfullscreen></iframe>


{% include figure.html path="assets/img/2022-12-01-LIME/LIME_.PNG" class="img-fluid" %}

## LIME code introduction

An interpretable explanation should use a representation that is understandable to humans, by explaining which parts of the input influence the model decision (regardless if its the raw input or a processed version of it). For example, for an image pixel-based explanations are not very convenient adn we can make use of [super-pixels](https://infoscience.epfl.ch/record/149300) which is a methodology that groups similar pixels together.Hence, a possible interpretable representation for image classification may be a binary vector indicating the <em>presence</em> or <em>absence</em> of a a super-pixel. Note, that the <em>black-box</em> still make use of the raw pixel for the classification and not the super-pixels.

We can denote $x \in R^{d}$ the original representation of an instance
being explained, and we use $x \in {0, 1}^{d}$ to denote a binary vector for its interpretable representation.

Formally, we define an explanation as a model $g \in G$,
where $G$ is a class of potentially interpretable models, such
as linear models, decision trees etc. THe explainer
model  $g \in G$ can be readily presented to the user with visual
or textual artifacts.

As not every $g \in G$ may be simple enough to be interpretable -
thus we let $\Omega(g)$ be a measure of complexity (as opposed to
interpretability) of the explanation $g \in G$. For example, for
decision trees $\Omega(g)$ may be the depth of the tree, while for
linear models, $\Omega(g)$ may be the number of non-zero weights.
Let the model being explained be denoted $f : R^{d} \to R$. In classification, $f(x)$ is the probability (or a binary indicator)
that $x$ belongs to a certain class. 

We further use $\pi_{x}(z)$ as a
proximity measure between an instance $z$ to $x$, so as to define
locality around $x$. Finally, let $\mathcal{L}(f, g, \pi_{x})$ be a measure of
how unfaithful $g$ is in approximating f in the locality defined
by $\pi_{x}$. In order to ensure both interpretability and local
fidelity, we must minimize $L(f, g, \pi_{x})$ while having $\Omega(g)$ be
low enough to be interpretable by humans. 

The explanation produced by LIME is obtained by the following:

$$\xi(x) = \mathcal{L}(f, g, \pi_{x}) + \Omega(g) $$

The above equation contains the tradeoff between local fidelity it is extrpressed by $L$ and complexity that it is expresseb by $\Omega$.

The first tem $\mathcal{L}(f, g, \pi_{x})$ in the paper is represented by the weighted square loss:

$$\mathcal{L}(f, g, \pi_{x}) = \sum_{z, z^{'}}\pi_{x}(z)(f(z)- g(z^{'}))^{2} $$

with $\pi_{x}$ to be a kernel function that measures the proximity of $z$ to $x$:

$$ \pi_{x} =  \exp(-D(x,z)^{2}/\sigma*{2})$$


{% include figure.html path="assets/img/2022-12-01-LIME/algorithm.png" class="img-fluid" %}


<center>
<video autoplay muted loop controls src="{{ site.url }}{{ site.baseurl }}/assets/video/2022-12-01-LIME/LIME.mp4"
    style="width:600px"
    type="video/mp4">
</video>
<figcaption>Visualization of the HalfCheetah agents learned through RCPPO and with different selected Lagrangian multipliers.</figcaption>
</center>


### Imports 

```python
import matplotlib.pyplot as plt
from PIL import Image
import torch.nn as nn
import numpy as np
import os, json
import cv2
os.environ['KMP_DUPLICATE_LIB_OK']='True'

import torch
from torchvision import models, transforms
from torch.autograd import Variable
import torch.nn.functional as F
import copy

import sklearn
import sklearn.metrics
from sklearn.linear_model import LinearRegression
```

### Initialization of a VGG19 model
A pre-trained VGG19 model is used to predict the class of the image. The output of the classification is a vector of 1000 proabilities of beloging to each class available in VGG19.


```python	
# load model
# model_type = 'vgg19'
model = models.vgg19(pretrained=True)

# run it on a GPU if available:
cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print('cuda:', cuda, 'device:', device)
model = model.to(device)
# set model to evaluation
model.eval()
```

Load our test image and see how it looks.

```python 
def imread_img(file_name):
  
  # read the image and convert it - Set your pathto the image
  img = cv2.imread('elephant-zebra.png')
  #img = cv2.imread(datafiles+ 'R.png')
  #img = cv2.imread(datafiles+ 'elephant/Elephant2.jpeg')
  # img = cv2.imread(datafiles+ 'shark/Shark1.jpeg')
  if (type(img) is np.ndarray):
    img = cv2.resize(img, (224, 224))
    img = img.astype(np.float32)
    img = img[:, :, (2, 1, 0)]
    print('img:', img.shape)
  else:
    print('image not found - set your path to the image')
  
  return img
```


```python
img = imread_img('elephant-zebra.png')
print (type(img))
print (img.shape) 
```

### Image pre-processing

```python
def pre_processing(obs, cuda):
    # Students should transpose the image to the correct tensor format. 
    # Students should ensure that gradient for input is calculated       
    # set the GPU device
    if cuda:
        torch_device = torch.device('cuda:0')
    else:
        torch_device = torch.device('cpu')

    # normalise for ImageNet
    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])
    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])
    obs = obs / 255
    obs = (obs - mean) / std

    # make tensor format that keeps track of gradient
    # BEGIN for students to do
    obs = np.transpose(obs, (2, 0, 1))       
    obs = np.expand_dims(obs, 0)
    obs = np.array(obs)
    obs_tensor = torch.tensor(obs, dtype=torch.float32, device=torch_device)
    # END for students to do
    return obs_tensor
```

Image prediction using the pre-trained VGG19 classifier:

```python 
def predict(input, model, target_label_idx, cuda):
    # Makes prediction after preprocessing image 
    # Note that output should be torch.tensor on cuda
    output = model(input)                        
    output = F.softmax(output, dim=1) # calc output from model 
    if target_label_idx is None:
      target_label_idx = torch.argmax(output, 1).item()
    index = np.ones((output.size()[0], 1)) * target_label_idx
    index = torch.tensor(index, dtype=torch.int64) 
    if cuda:
      index = index.cuda()                     # calc prediction
    output = output.gather(1, index)           # gather functionality of pytorch
    return target_label_idx, output 

# test preprocessing
# you can check that the VGG network gives a correct prediction. E.g. 385 and 386 are 'Indian Elephant'and 'African Elephant'
input = pre_processing(img, cuda)          # preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
print (input.shape)
label, output = predict(input, model, None, cuda)
print('output:', output)
print('output label:', label)
```



```python
# resize and take the center part of image to what our model expects
def get_input_transform():
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                    std=[0.229, 0.224, 0.225])       
    transf = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        normalize
    ])    

    return transf

def get_input_tensors(img):
    transf = get_input_transform()
    # unsqeeze converts single image to batch of 1
    return transf(img).unsqueeze(0)
```

```python
idx2label, cls2label, cls2idx = [], {}, {}
with open(os.path.abspath('imagenet_class_index.json'), 'r') as read_file:
    class_idx = json.load(read_file)
    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]
    cls2label = {class_idx[str(k)][0]: class_idx[str(k)][1] for k in range(len(class_idx))}
    cls2idx = {class_idx[str(k)][0]: k for k in range(len(class_idx))} 
```

Get the predicition for our image.

```python
def get_image(path):
    with open(os.path.abspath(path), 'rb') as f:
        with Image.open(f) as img:
            return img.convert('RGB') 
        
img = get_image('elephant-zebra.png')
img_t = get_input_tensors(img)
model.eval()
logits = model(img_t)


probs = F.softmax(logits, dim=1)
probs5 = probs.topk(5)
tuple((p,c, idx2label[c]) for p, c in zip(probs5[0][0].detach().numpy(), probs5[1][0].detach().numpy()))
```


## LIME explanation 
The following figure illustrates the basic idea behind LIME. The figure shows light and dark gray areas which are the decision boundaries for the classes for each (x1,x2) pairs in the dataset. LIME is able to provide explanations for the predictions of an individual record (blue dot). The  explanations are created by generating a new dataset of perturbations around the instance to be explained (colored markers around the blue dot). The output or class of each generated perturbation is predicted with the machine-learning model (colored markers inside and outside the decision boundaries). The importance of each perturbation is determined by measuring its distance from the original instance to be explained. These distances are converted to weights by mapping the distances to a zero-one scale using a kernel function (see color scale for the weights). All this information: the new generated dataset, its class predictions and its weights are used to fit a simpler model, such as a linear model (blue line), that can be interpreted. The attributes of the simpler model, coefficients for the case of a linear model, are then used to generate explanations.  

{% include figure.html path="assets/img/2022-12-01-LIME/LIME.PNG" class="img-fluid" %}

A detailed explanation of each step is shown below.

## Creating Perturbations of image

For the case of image explanations, perturbations will be generated by turning on and off some of the superpixels in the image.

#### Extract super-pixels from image
Superpixels are generated using the quickshift segmentation algorithm. It can be noted that for the given image, 68 superpixels were generated. The generated superpixels are shown in the image below.

```python
import skimage.io 
import skimage.segmentation

img = skimage.io.imread("elephant-zebra.png")
img = skimage.transform.resize(img, (224,224)) 
img = (img - 0.5)*2 #Inception pre-processing
skimage.io.imshow(img/2+0.5) # Show image before inception preprocessing


superpixels = skimage.segmentation.quickshift(img, kernel_size=4,max_dist=200, ratio=0.2)
num_superpixels = np.unique(superpixels).shape[0]
num_superpixels

skimage.io.imshow(skimage.segmentation.mark_boundaries(img/2+0.5, superpixels))
```

### Creating random perturbations
In this example, 150 perturbations were used. However, for real life applications, a larger number of perturbations will produce more reliable explanations. Random zeros and ones are generated and shaped as a matrix with perturbations as rows and superpixels as columns. An example of a perturbation (the first one) is show below. Here, `1` represent that a superpixel is on and `0` represents it is off. Notice that the length of the shown vector corresponds to the number of superpixels in the image.

```python
num_perturb = 150
perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))
perturbations[0] #Show example of perturbation

```	
The following function `perturb_image` perturbs the given image (`img`) based on a perturbation vector (`perturbation`) and predefined superpixels (`segments`).

```python
def perturb_image(img,perturbation,segments):
  active_pixels = np.where(perturbation == 1)[0]
  mask = np.zeros(segments.shape)
  for active in active_pixels:
      mask[segments == active] = 1 
  perturbed_image = copy.deepcopy(img)
  perturbed_image = perturbed_image*mask[:,:,np.newaxis]
  return perturbed_image
```

Let's use the previous function to see what a perturbed image would look like:

```python 
skimage.io.imshow(perturb_image(img/2+0.5,perturbations[0],superpixels))
```

### Step 2: Use ML classifier to predict classes of new generated images
This is the most computationally expensive step in LIME because a prediction for each perturbed image is computed. From the shape of the predictions we can see for each of the perturbations we have the output probability for each of the 1000 classes in Inception V3. 

```python	
predictions = []
for pert in perturbations:
  perturbed_img = perturb_image(img,pert,superpixels)
  input = pre_processing(perturbed_img, cuda)   
  # preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
  output, label = predict(input, model, None, cuda)
  
  output = model(input)                        
  output = F.softmax(output, dim=1)
  print (output.shape)
  target_label_idx = torch.argmax(output, 1).item()
  
  predictions.append(output.detach().numpy())

predictions = np.array(predictions)
predictions.shape
```


```python
original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled 
distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()
distances.shape
```


```python	
kernel_width = 0.25	
weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2)) #Kernel function 
weights.shape
```

```python
img = imread_img('elephant-zebra.png')

input = pre_processing(img, cuda)          # preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)

out = model(input)     
print (out.shape)                
out = F.softmax(out, dim=1)    

out, indices= torch.sort(out, descending=True)

top_values = out[:, :5] # Keep the first 5 values from each row
top_indices = indices[:, :5]   # Keep the corresponding indices

top5 = torch.topk(out, k=5)

topk_values = top_values.detach().numpy()
topk_indices =  top_indices.detach().numpy()

print(topk_values)
print(topk_indices)
```

```python	
simpler_model = LinearRegression()
# print (topk_indices[0][0])
# print (perturbations.shape)
# print (predictions[:,:,topk_indices[0][0]])
simpler_model.fit(X=perturbations, y=predictions[:,:,topk_indices[0][0]], sample_weight=weights)
coeff = simpler_model.coef_[0]
coeff
```

```python	
num_top_features = 4
top_features = np.argsort(coeff)[-num_top_features:] 
top_features
```

```python	
mask = np.zeros(num_superpixels) 
mask[top_features]= True #Activate top superpixels

img = imread_img('elephant-zebra.png')

img = img/255
skimage.io.imshow(perturb_image(img ,mask,superpixels) )
```



<!-- &nbsp;  
<callout>
Google and Meta have both developed advanced AI networks that can generate new, unseen videos using only simple text prompts. Try clicking through the prompts and compare the results between Google's Imagen Video and Meta's Make-a-Video models:
</callout>
<figure1>
  <iframe height="600px" width="840px" scrolling="No" frameborder="0" hspace="0" vspace="0" src="https://video-gui.onrender.com/"></iframe>
</figure1> -->

<!-- In this post, we dissect and explain the mechanics behind the key building blocks for state-of-the-art Text-to-Video generation. We provide interactive examples of these building blocks and demonstrate the key novelties/differences between two Text-to-Video models: Imagen Video and Make-a-Video. Finally, we summarize by showing how the building blocks fit together into a complete Text-to-Video framework as well as noting the current failure modes and limitations of the models today.
{: style="text-align: justify"}

# History of Text-to-Video
Just six months after the release of DALL-E 2, both Meta and Google released novel Text-to-Video generation models that output impressive video-format content. These networks build off of recent advancements in Text-to-Image modeling using stable diffusion (like DALL-E [[1]](https://arxiv.org/pdf/2102.12092) and Imagen [[2]](https://arxiv.org/pdf/2205.11487)). Meta’s Make-A-Video [[3]](https://arxiv.org/pdf/2209.14792) is capable of five second 768x768 clips at variable frame rates while Google’s Imagen Video [[4]](https://arxiv.org/pdf/2210.02303) can produce 1280×768 videos at 24 fps. Rather than training strictly on text-video pair datasets, both Imagen Video and Make-a-Video leverage the massive text-image pair databases to construct video from pretrained Text-to-Image generation models. These Text-to-Video generators are capable of creating high-resolution, photorealistic and stylistic content of impossible scenarios. Networks such as these can be powerful tools for artists and creators as well as the basis for predicting future frames of a video.
{: style="text-align: justify"}


## Limitations of Text-to-Video
As beautiful as many of these videos are . . .
{: style="text-align: justify"}

<figure>
  <video autoplay muted loop controls src="https://imagen.research.google/video/hdvideos/51.mp4" width="600" type="video/mp4">
  </video>
</figure>
&nbsp;  

Not all of them are perfect . . . *(pay close attention to the legs of the elephant walking)*
{: style="text-align: justify"}

<figure>
  <video autoplay muted loop controls src="https://imagen.research.google/video/hdvideos/14.mp4" width="600" type="video/mp4">
  </video>
</figure>

Although Imagen Video and Make-a-Video have made significant progress in temporal coherency to remove flickering effects, complex videos generated where image data is sparse, have poor realism across the temporal dimension. In the elephant walking underwater example, a lack of training data of elephants walking or perhaps training sets with insufficient frame rates results in latent diffusion having to work harder to interpolate the missing frames, resulting in **poor temporal realism**. However, as both datasets and models continue to grow in size, the videos generated by the methods discussed in this post will improve in realism and these failure modes will become less common.

Furthermore, both models are optimized for producing shorter (5-second) videos. Since Make-A-Video directly builds on Text-to-Image, it cannot learn associations that can only be learned from videos. Longer videos containing multiple scenes and actions are challenging to generate with both of these models.

Undoubtedly, these Text-to-Video generation methods can substantially expand the creative toolbox available to artists and creators, however, key issues should be addressed before these networks become publicly available. For example, misuse of the models can result in fake, explicit, hateful, or otherwise generally **harmful content**. To help address this, additional classifiers can be trained to filter text inputs and video outputs. Moreover, the outputs reflect the composition of the training dataset, which include some problematic data, social biases, and stereotypes.
{: style="text-align: justify"} -->

# Related Works
Several advancements have been achieved with the methods described in this post, however, video generation is not a new concept, nor do the methods described in this post solve all video generation challenges. So, here is a selection of some other interesting video generation variations/applications developed by other researchers:
{: style="text-align: justify"}
* [Phenaki](https://phenaki.video/) is another video generation tool that can generate videos of several minutes in length from story-like text prompts, compared to 5 second videos generated by Imagen Video and Make-a-Video.
* [Lee *et al.*](https://kuai-lab.github.io/eccv2022sound/) and [Narashimhan *et al.*](https://medhini.github.io/audio_video_textures/) generated video synced with audio inputs.
* [Visual Foresight](https://sites.google.com/view/visualforesight?pli=1) predicts how an object will move given an action in pixel space for more practical robotics planning and control applications.

# References
[[1] Ramesh, A. et al. Zero-Shot Text-to-Image Generation, 2021. *arXiv Preprint*.](https://arxiv.org/pdf/2102.12092)
{: style="font-size: smaller"}

[[2] Saharia, C. et al. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, 2022. *arXiv Preprint*.](https://arxiv.org/pdf/2205.11487)
{: style="font-size: smaller"}

[[3] Singer, U. et al. Make-a-Video: Text-to-Video Generation Without Text-Video Data, 2022. *arXiv Preprint*.](https://arxiv.org/pdf/2209.14792)
{: style="font-size: smaller"}

[[4] Ho, J. et al. Imagen Video: High Definition Video Generation with Diffusion Models, 2022. *arXiv Preprint*.](https://arxiv.org/pdf/2210.02303)
{: style="font-size: smaller"}

[[5] Finn, C. et al. Unsupervised Learning for Physical Interaction through Video Prediction, 2016. *30th Conference on Neural Information Processing Systems (NeurIPS)*.](https://proceedings.neurips.cc/paper/2016/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf)
{: style="font-size: smaller"}

[[6] Wang, Y. et al. PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs, 2017. *30th Conference on Neural Information Processing Systems (NeurIPS)*.](https://papers.nips.cc/paper/2017/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf)
{: style="font-size: smaller"}

[[7] Babaeizadeh, M. et al. Stochastic Variational Video Prediction, 2018. *International Conference on Learning Representations (ICLR)*.](https://openreview.net/pdf?id=rk49Mg-CW)
{: style="font-size: smaller"}

[[8] Zhai, S. et al. Generative Adversarial Networks as Variational Training of Energy Based Models, 2017. *arXiv Preprint*.](https://arxiv.org/pdf/1611.01799.pdf)
{: style="font-size: smaller"}

[[9] Saito, M. et al. Temporal Generative Adversarial Nets with Singular Value Clipping, 2016. *arXiv Preprint*.](https://arxiv.org/pdf/1611.06624)
{: style="font-size: smaller"}

[[10] Wu, C. et al. GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions, 2021. *arXiv Preprint*.](https://arxiv.org/pdf/2104.14806)
{: style="font-size: smaller"}

[[11] Wu, C. et al. NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion, 2021. *arXiv Preprint*.](https://arxiv.org/pdf/2111.12417)
{: style="font-size: smaller"}

[[12] Hong, W. et al. CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers, 2022. *arXiv Preprint*.](https://arxiv.org/pdf/2205.15868)
{: style="font-size: smaller"}

[[13] Kingma, D. P. et al. Variational Diffusion Models, 2021. *35th Conference on Neural Information Processing Systems (NeurIPS)*.](https://openreview.net/pdf?id=2LdBqxc1Yv)
{: style="font-size: smaller"}

[[14] Ding, M. et al. CogView: Mastering Text-to-Image Generation via Transformers, 2021. *35th Conference on Neural Information Processing Systems (NeurIPS)*.](https://proceedings.neurips.cc/paper/2021/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf)
{: style="font-size: smaller"}

[[15] Rombach, R. et al. High-Resolution Image Synthesis with Latent Diffusion Models, 2022. *IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)*.](https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf)
{: style="font-size: smaller"}

[[16] Ronneberger, O. et al. U-Net: Convolutional Networks for Biomedical Image Segmentation, 2015. *arXiv Preprint*.](https://arxiv.org/pdf/1505.04597)
{: style="font-size: smaller"}
