---
layout: distill
title: Why should I trust you? LIME for image classification
description: This page's goal is to present a PostHoc feature attribution XAI methodology called LIME (Local Interpretable Model-agnostic Explanations) and how it can be used to explain image classification tasks. You will be guided through the code and the results of the LIME algorithm. Part of the assessemnet for this workshop, will be some research questions that needs be answered by you. These questions can be found all over this blogspot using the <mark>TOSUBMIT</mark> tag and will be summarized them at the end of the blogspot.
date: 2022-12-01
htmlwidgets: true

# anonymize when submitting
authors:
  - name: Anonymous

# do not fill this in until your post is accepted and you're publishing your camera-ready post!
authors:
  - name: Christos Athanasiadis
    url: "https://www.linkedin.com/in/christos-athanasiadis-a3b51035/"
    affiliations:
      name: UvA, Interpretability and Explainability in AI

# must be the exact same name as your blogpost
# bibliography: 2022-12-01-distill-example.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Local Interpretable Model-agnostic Explanations
    subsections:
    - name: Interpretable Representations
    - name: LIME approach details
    - name: Code implementation
    - name: LIME explanation 
    - name: Creating Perturbations of image
  - name: Results
  - name: Adversarial attacks
  - name: TOSUBMIT
  - name: Conclusions
  - name: References
---

# Local Interpretable Model-agnostic Explanations



<center>
<video autoplay muted loop controls src="{{ site.url }}{{ site.baseurl }}/assets/video/2022-12-01-LIME/labradors.mp4"
    style="width:600px"
    type="video/mp4">
</video>
<figcaption>A demonstration of the whole LIME algorithm.</figcaption>
</center> 

In this post, we will study how <mark style="background-color:LavenderBlush;">LIME</mark> (Local Interpretable Model-agnostic Explanations) ([1]) generates explanations for image classification tasks. The basic idea is to understand why a machine learning model predicts that an specific image belongs to a certain class (<em>labrador</em> our visual example). The LIME explainer is <em>model-agnostic</em> that means is not restricted to a specific model and can be used to explain any <mark>black-box</mark> classifier. So we dont need to have access to the details of our model (input, intermediate layers etc) to generate explanations. Moreover, the explainer is <em>local</em> meaning that it explains the prediction of the model in the neighborhood of the instance being explained. This technique lies in the PostHoc category of XAI methods, meaning that it explains the model after it has been trained. Briefly, this technique tried to learn a simple model (for isntance a linear classifier) that is interpretable by humans and approximates the predictions of the <em>black-box</em> model in the neighborhood of the instance being explained.

## Interpretable Representations

An interpretable explanation in image classifier should use a representation that is understandable to humans, by explaining which parts of the input image influence the model decision. For example, pixel-based explanations are not very informative especially when we deal with huge images and therefore a better way to explain the model decision is to use [super-pixels](https://infoscience.epfl.ch/record/149300). Super-pixels are groups of pixels that share similar characteristics such as color and texture. Hence, a possible interpretable representation for image classification may be a binary vector indicating the <em>presence</em> or <em>absence</em> of a super-pixel. Thus, our explainer needs to find a way to attribute importance to each super-picel in the initial input image. Its important to note here, that the interpretable representations are meant to be just for the explainer while the <em>black-box</em> can still be trained using the original pixel-based representation.

<mark style="background-color:LightCyan;">LIME</mark> approach aims to just give an explanation why the classifier took a specific decision upon a specific input image. It does not aim to explain the whole model. Authors in the paper proposed a mechanism called <mark style="background-color:LightCyan;">SP-LIME</mark> that aims to explain the whole model. While we will not touch this method in this tutorial we encourage you to have a look at it in the original paper.

## LIME approach details

To explain how LIME works we will need to introduce some terminology ðŸ˜Ž. Hence, let $\mathbf{x} \in R^{d}$ denote the original vector representation of an instance
being explained (in our case a vector with all pixels), and we use $\mathbf{x}^{'} \in {0, 1}^{d}$ to denote a binary vector for its interpretable representation (super-pixels).

Authors, then define as an explainer (or explanation model) $g \in G$, where $G$ is a class of potentially interpretable models, such as <em>linear models</em>, <em>decision trees</em> etc. To keep things simple, in this tutorial, we will consider just <em>linear classifiers</em>. As not every $g \in G$ may be simple enough to be interpretable thus we let $\Omega(g)$ be a measure of complexity (as opposed to interpretability) of the explanation $g \in G$. For example, for decision trees $\Omega(g)$ may be the depth of the tree, while for linear models, $\Omega(g)$ may be the number of non-zero weights. Authors define as $f : R^{d} \to R$ the <em>black-box</em> model that would like to explain. In classification, $f(\mathbf{x})$ is the probability (or a binary indicator) that $\mathbf{x}$ belongs to a certain class. 

They further use $\pi_{\mathbf{x}}(\mathbf{z})$ as a proximity measure between an instance $\mathbf{z}$ to $\mathbf{x}$, so as to define locality around $\mathbf{x}$. Finally, let $\mathcal{L}(f, g, \pi_{\mathbf{x}})$ be a measure of how unfaithful $g$ is in approximating $f$ in the locality defined by $\pi_{\mathbf{x}}$. In order to ensure both <em>interpretability</em> and <em>local fidelity</em>, they must minimize $L(f, g, \pi_{x})$ while having $\Omega(g)$ be low enough to be interpretable by humans. 

The explanation produced by LIME is obtained by the following:

$$\xi(\mathbf{x}) = \mathcal{L}(f, g, \pi_{\mathbf{x}}) + \Omega(g) $$

The above equation contains the tradeoff between <en>local fidelity</em> that it is extrpressed by $L$ and <em>complexity</em> that it is expressed by $\Omega$.

The first tem $\mathcal{L}(f, g, \pi_{\mathbf{x}})$ in the paper is represented by the weighted square loss:

$$\mathcal{L}(f, g, \pi_{\mathbf{x}}) = \sum_{\mathbf{z}, \mathbf{z}^{'}}\pi_{\mathbf{x}}(\mathbf{z})(f(\mathbf{z})- g(\mathbf{z}^{'}))^{2} $$

with $\pi_{\mathbf{x}}$ to be a kernel function that measures the proximity of $z$ to $x$:

$$ \pi_{\mathbf{x}} =  \exp(-D(\mathbf{x},\mathbf{z})^{2}/\sigma*{2})$$

The idea is that tuning the weights $\mathbf{w}$ we can use them directly as a feature attribution to each super-pixel. The higher the weight that corresponds to a specific super-pixel the more important this super-pixel is for the prediction of the <em>black-box</em> model and the vice-versa.

Another importance explanation is about the terms <em>faithfullness</em> and <em>local-fidelity</em> and they about how well our explainer $g$ can approximate the decision of the <em>black-box</em> model $f$ in the locality defined by $\pi_{\mathbf{x}}$.

The whole LIME algorithm can be summarized as follows:

{% include figure.html path="assets/img/2022-12-01-LIME/algorithm.png" class="img-fluid" %}

The kernel is the distance between the instance being explained and the instance in the neighborhood. The neighborhood is generated by <mark style="background-color:LightCyan;">sampling</mark> instances around the instance being explained. The sampling is done by perturbing the instance being explained. For example, in the case of images, we can perturb the image by zeroing out some super-pixels. The perturbed instances are then fed to the <em>black-box</em> model and the output is used to train the explainer. The weights of the interpretable model are then used to explain the prediction of the <em>black-box</em> model. Finally, in the algorithm the <mark style="background-color:Lavender;">K-lasso</mark> refers to the regulizarion that is introduced in a previous equation and relates with the term $\Omega(g)$.

<center>
<video autoplay muted loop controls src="{{ site.url }}{{ site.baseurl }}/assets/video/2022-12-01-LIME/LIME.mp4"
    style="width:600px"
    type="video/mp4">
</video>
<figcaption>A demonstration of the whole LIME algorithm.</figcaption>
</center> 

The above video explain the whole LIME process. The initial surface represents the <mark style="background-color:Lavender;">black-box</mark> classifier and the regions for the class of interest (e.g. basketball with the light-pink color). The dark-colored dot denotes the sample that we would like to explain and it is actually an image with the label <em> basketball</em>. The first step is to sample the neighborhood of the point $\mathbf{x}$ that we would like to explain. Several points are generated. The size of each generated sample and the trasparency relates with the distance from the initial point $\mathbf{x}$ which is calculated based using $\pi_{\mathbf{x}}(\mathbf{z})$. The next step is to apply the <mark style="background-color:Lavender;">black-box</mark> classifier $f()$ to find the label for each generated point. Samples with red represetns class basketball while samples with purple represents class not basketball. The next step is to train the interpretable model $g()$ using the generated samples. The weights of the interpretable model are used to explain the prediction of the <mark style="background-color:Lavender;">black-box</mark> classifier. The explanation is actually a linear classifier that separates the two classes. The weights of the linear classifier can be used as an explanation for the whole approach.


## Code implementation

Firstly, we will need to import the required libraries. The code is written in Python 3.6.9 and PyTorch 1.7.0. The code is available in the following [link](TBA)
### Imports 

```python
import matplotlib.pyplot as plt
from PIL import Image
import torch.nn as nn
import numpy as np
import os, json
import cv2
os.environ['KMP_DUPLICATE_LIB_OK']='True'

import torch
from torchvision import models, transforms
from torch.autograd import Variable
import torch.nn.functional as F
import copy

import sklearn
import sklearn.metrics
from sklearn.linear_model import LinearRegression
```

### Initialization of a VGG19 model
A pre-trained VGG19 model is used to predict the class of the image. The output of the classification is a vector of 1000 proabilities of beloging to each class available in VGG19. The model is initialized and the weights are loaded. The model is set to evaluation mode. The model is set to run on GPU if available.

```python	
# load model
# model_type = 'vgg19'
model = models.vgg19(pretrained=True)

# run it on a GPU if available:
cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print('cuda:', cuda, 'device:', device)
model = model.to(device)
# set model to evaluation
model.eval()
```
Ignore for now the warnings! This code should return the architecture of the VGG19 model. Of course, feel-free to choose the model of you choice the code should work with any model.
Now lets load and process the image (for the VGG19 classifier) that we would like to test our <mark style="background-color:Lavender;">LIME</mark> explainer. You can freely choose your own image that you would like to explain.

```python 
def imread_img(file_name):
  
  # read the image and convert it - Set your pathto the image
  img = cv2.imread('caretta.png')
  if (type(img) is np.ndarray):
    img = cv2.resize(img, (224, 224))
    img = img.astype(np.float32)
    img = img[:, :, (2, 1, 0)]
    print('img:', img.shape)
  else:
    print('image not found - set your path to the image')
  
  return img
```

### Image pre-processing

As usual, we will need to normalize our input image. The normalization is done using the mean and standard deviation of the ImageNet dataset. The image is also transposed to the correct tensor format:

```python
def pre_processing(obs, cuda):
    # Students should transpose the image to the correct tensor format. 
    # Students should ensure that gradient for input is calculated       
    # set the GPU device
    if cuda:
        torch_device = torch.device('cuda:0')
    else:
        torch_device = torch.device('cpu')

    # normalise for ImageNet
    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])
    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])
    obs = obs / 255
    obs = (obs - mean) / std

    # make tensor format that keeps track of gradient
    # BEGIN for students to do
    obs = np.transpose(obs, (2, 0, 1))       
    obs = np.expand_dims(obs, 0)
    obs = np.array(obs)
    obs_tensor = torch.tensor(obs, dtype=torch.float32, device=torch_device)
    # END for students to do
    return obs_tensor
```

Then, we check the prediction by using the VGG19 model that we loaded before. You can check that the VGG network gives a correct prediction. E.g. 33 and 34 are 'caretta-caretta'and 'turtle'

```python 
def predict(input, model, target_label_idx, cuda):
    # Makes prediction after preprocessing image 
    # Note that output should be torch.tensor on cuda
    output = model(input)                        
    output = F.softmax(output, dim=1) # calc output from model 
    if target_label_idx is None:
      target_label_idx = torch.argmax(output, 1).item()
    index = np.ones((output.size()[0], 1)) * target_label_idx
    index = torch.tensor(index, dtype=torch.int64) 
    if cuda:
      index = index.cuda()                     # calc prediction
    output = output.gather(1, index)           # gather functionality of pytorch
    return target_label_idx, output 

input = pre_processing(img, cuda)          # preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
print (input.shape)
label, output = predict(input, model, None, cuda)
print('output:', output)
print('output label:', label)
```



```python
# resize and take the center part of image to what our model expects
def get_input_transform():
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                    std=[0.229, 0.224, 0.225])       
    transf = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        normalize
    ])    

    return transf

def get_input_tensors(img):
    transf = get_input_transform()
    # unsqeeze converts single image to batch of 1
    return transf(img).unsqueeze(0)
```

The following code helps you to get the label of the prediction. The label is the index of the class in the ImageNet dataset. The index is used to get the class name from the json file. The json file is available in the repository.

```python
idx2label, cls2label, cls2idx = [], {}, {}
with open(os.path.abspath('imagenet_class_index.json'), 'r') as read_file:
    class_idx = json.load(read_file)
    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]
    cls2label = {class_idx[str(k)][0]: class_idx[str(k)][1] for k in range(len(class_idx))}
    cls2idx = {class_idx[str(k)][0]: k for k in range(len(class_idx))} 
```

## LIME explanation 
The following figure illustrates the basic idea behind LIME. The figure shows with light pink and blue areas which are the decision boundaries for the classifier (for the VGG19 pre-trained model on ImageNet for instance). <mark style="background-color:Lavender;">LIME</mark> is able to provide explanations for the predictions of an individual instance (the one with the dark red dot). These explanations are created by generating a new dataset of perturbations around the instance to be explained (in our image are depicted with dot circles around the initial instance). 

Then, we apply our <mark style="background-color:Lavender;">black-box</mark> model $g()$ and we can extract the label for all the pertubations (and can be seen which red that denotes turtle and purple that denotes non-turtle). The <em>importance</em> of each perturbation is determined by measuring its distance from the original instance to be explained. These distances are converted to weights by mapping the distances to a zero-one scale using a kernel function (see color scale for the weights). All this information: the new generated dataset, its class predictions and its weights are used to fit a simpler model, such as a linear model (gray line), that can be interpreted. The coefficients for the case of a linear model, are then extracted and used as the explanation for the prediction of the instance to be explained. The higher the coefficient, the more important the feature is for the prediction.  

{% include figure.html path="assets/img/2022-12-01-LIME/LIME.png" class="img-fluid" %}

## Creating Perturbations of image

For the case of image explanations, perturbations will be generated by turning on and off some of the superpixels in the image.

#### Extract super-pixels from image
Superpixels are generated using the quickshift segmentation algorithm. It can be noted that for the given image, <mark style="background-color:Lavender;">68</mark> superpixels were generated. That of course will change if you will update the <mark style="background-color:Lavender;">quickshift</mark> parameters (<mark style="background-color:Lavender;">kernel_size</mark> and <mark style="background-color:Lavender;">max_dist</mark>). The generated superpixels are shown in the image below.

```python
import skimage.io 
import skimage.segmentation

superpixels = skimage.segmentation.quickshift(img/255, kernel_size=6, max_dist=200, ratio=0.2)
num_superpixels = np.unique(superpixels).shape[0]
num_superpixels

imgplot = plt.imshow(skimage.segmentation.mark_boundaries(img/255, superpixels))
plt.axis('off')
plt.show()

```

{% include figure.html path="assets/img/2022-12-01-LIME/caretta_2.jpg" class="img-fluid" %}

### Creating random perturbations
In this example, 150 perturbations were used. However, for real life applications, a larger number of perturbations will produce more reliable explanations. Random zeros and ones are generated and shaped as a matrix with perturbations as rows and superpixels as columns. An example of a perturbation (the first one) is show below. Here, `1` represent that a superpixel is on and `0` represents it is off. Notice that the length of the shown vector corresponds to the number of superpixels in the image.

```python
num_perturb = 150
perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))
perturbations[0] #Show example of perturbation

```	
The following function `perturb_image` perturbs the given image (`img`) based on a perturbation vector (`perturbation`) and predefined superpixels (`segments`).

```python
def perturb_image(img,perturbation,segments):
  active_pixels = np.where(perturbation == 1)[0]
  mask = np.zeros(segments.shape)
  for active in active_pixels:
      mask[segments == active] = 1 
  perturbed_image = copy.deepcopy(img)
  perturbed_image = perturbed_image*mask[:,:,np.newaxis]
  return perturbed_image
```

Let's use the previous function to see what a perturbed image would look like:

```python 
skimage.io.imshow(perturb_image(img/2+0.5,perturbations[0],superpixels))
```

{% include figure.html path="assets/img/2022-12-01-LIME/caretta_3.jpg" class="img-fluid" %}

### Step 2: Use ML classifier to predict classes of new generated images
This is the most computationally expensive step in LIME because a prediction for each perturbed image is computed. From the shape of the predictions we can see for each of the perturbations we have the output probability for each of the 1000 classes in Inception V3. 

```python	
predictions = []
for pert in perturbations:
  perturbed_img = perturb_image(img,pert,superpixels)
  input = pre_processing(perturbed_img, cuda)   
  # preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)
  output, label = predict(input, model, None, cuda)
  
  output = model(input)                        
  output = F.softmax(output, dim=1)
  print (output.shape)
  target_label_idx = torch.argmax(output, 1).item()
  
  predictions.append(output.detach().numpy())

predictions = np.array(predictions)
predictions.shape
```


```python
original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled 
distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()
distances.shape
```


```python	
kernel_width = 0.25	
weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2)) #Kernel function 
weights.shape
```

```python
img = imread_img('elephant-zebra.png')

input = pre_processing(img, cuda)          # preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)

out = model(input)     
print (out.shape)                
out = F.softmax(out, dim=1)    

out, indices= torch.sort(out, descending=True)

top_values = out[:, :5] # Keep the first 5 values from each row
top_indices = indices[:, :5]   # Keep the corresponding indices

top5 = torch.topk(out, k=5)

topk_values = top_values.detach().numpy()
topk_indices =  top_indices.detach().numpy()

print(topk_values)
print(topk_indices)
```

```python	
simpler_model = LinearRegression()
# print (topk_indices[0][0])
# print (perturbations.shape)
# print (predictions[:,:,topk_indices[0][0]])
simpler_model.fit(X=perturbations, y=predictions[:,:,topk_indices[0][0]], sample_weight=weights)
coeff = simpler_model.coef_[0]
coeff
```

```python	
num_top_features = 4
top_features = np.argsort(coeff)[-num_top_features:] 
top_features
```

```python	
mask = np.zeros(num_superpixels) 
mask[top_features]= True #Activate top superpixels

img = imread_img('elephant-zebra.png')

img = img/255
skimage.io.imshow(perturb_image(img ,mask,superpixels) )
```

{% include figure.html path="assets/img/2022-12-01-LIME/caretta_4.jpg" class="img-fluid" %}


# Results

# Adversarial attacks

# TOSUBMIT

# Conclusions

# References
[[1] Ribeuro, M.T. et al. Why Should I Trust You? Explaining the Predictions of Any Classifier, 2016. SIGKDD.](https://arxiv.org/pdf/1602.04938.pdf)
{: style="font-size: smaller"}


[[2] Saharia, C. et al. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, 2022. *arXiv Preprint*.](https://arxiv.org/pdf/2205.11487)
{: style="font-size: smaller"}

[[3] Singer, U. et al. Make-a-Video: Text-to-Video Generation Without Text-Video Data, 2022. *arXiv Preprint*.](https://arxiv.org/pdf/2209.14792)
{: style="font-size: smaller"}

[[4] Ho, J. et al. Imagen Video: High Definition Video Generation with Diffusion Models, 2022. *arXiv Preprint*.](https://arxiv.org/pdf/2210.02303)
{: style="font-size: smaller"}